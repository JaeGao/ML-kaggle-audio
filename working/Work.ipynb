{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Importing Libraries\n",
    "import tarfile\n",
    "import resampy\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping extraction of train_mp3s.tar.\n",
      "Skipping extraction of test_mp3s.tar.\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Extracting Data\n",
    "import shutil\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "def extract_tar(tar_file, target_dir):\n",
    "    if os.path.exists(target_dir):\n",
    "        user_input = input(f\"The directory '{target_dir}' already exists. Do you want to skip extraction? (y/n): \")\n",
    "        if user_input.lower() == 'y':\n",
    "            print(f\"Skipping extraction of {tar_file}.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Overwriting the existing directory '{target_dir}'.\")\n",
    "            shutil.rmtree(target_dir)\n",
    "\n",
    "    with tarfile.open(tar_file, 'r') as tar:\n",
    "        tar.extractall(target_dir)\n",
    "\n",
    "    # Remove residue \"._\" hidden files from the inner folder\n",
    "    inner_folder = os.path.join(target_dir, os.path.splitext(os.path.basename(tar_file))[0])\n",
    "    for root, dirs, files in os.walk(inner_folder):\n",
    "        for file in files:\n",
    "            if file.startswith(\"._\"):\n",
    "                os.remove(os.path.join(root, file))\n",
    "\n",
    "extract_tar('train_mp3s.tar', 'train_mp3s')\n",
    "extract_tar('test_mp3s.tar', 'test_mp3s')\n",
    "train_labels = np.loadtxt('train_label.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Preprocessing Functions\n",
    "def preprocess_audio(file_path):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        print(f\"Loaded audio file: {file_path}\")\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
    "        print(f\"Extracted MFCCs: {mfccs_scaled.shape}\")\n",
    "        return mfccs_scaled\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_file(file_path):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    mfccs = preprocess_audio(file_path)\n",
    "    return mfccs\n",
    "\n",
    "def prepare_data(directory):\n",
    "    file_paths = [os.path.join(directory, f\"{i}.mp3\") for i in range(len(os.listdir(directory)))]\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_file, file_paths))\n",
    "    \n",
    "    features = [mfccs for mfccs in results if mfccs is not None]\n",
    "    print(f\"Processed {len(features)} audio files\")\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: train_mp3s/train_mp3s\\0.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\1.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\2.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\3.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\4.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\5.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\6.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\7.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\8.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\9.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\10.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\11.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\12.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\13.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\14.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\15.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\16.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\17.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\18.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\19.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\20.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\21.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\22.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\23.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\24.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\25.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\26.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\27.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\28.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\29.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\30.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\31.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\0.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\19.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\16.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\21.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\15.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\5.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\24.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\23.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\27.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\4.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\1.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\18.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\28.mp3\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Preparing Data\n",
    "train_features = prepare_data('train_mp3s/train_mp3s')\n",
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "\n",
    "test_features = prepare_data('test_mp3s/test_mp3s')\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "train_labels = np.array([int(label) for label in train_labels])\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "\n",
    "print(f\"Number of training features: {len(train_features)}\")\n",
    "print(f\"Number of training labels: {len(train_labels)}\")\n",
    "print(f\"Number of test features: {len(test_features)}\")\n",
    "\n",
    "if len(train_features) == 0:\n",
    "    print(\"No training features available. Please check the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mojii\\anaconda3\\envs\\ML\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.3031 - loss: 16.0292 - val_accuracy: 0.3671 - val_loss: 1.2899\n",
      "Epoch 2/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3565 - loss: 1.5759 - val_accuracy: 0.3702 - val_loss: 1.2760\n",
      "Epoch 3/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3611 - loss: 1.3563 - val_accuracy: 0.3746 - val_loss: 1.2518\n",
      "Epoch 4/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3826 - loss: 1.3000 - val_accuracy: 0.3836 - val_loss: 1.2026\n",
      "Epoch 5/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3952 - loss: 1.2471 - val_accuracy: 0.4638 - val_loss: 1.1470\n",
      "Epoch 6/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4153 - loss: 1.1917 - val_accuracy: 0.5230 - val_loss: 1.0871\n",
      "Epoch 7/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4545 - loss: 1.1520 - val_accuracy: 0.5766 - val_loss: 1.0234\n",
      "Epoch 8/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4862 - loss: 1.1027 - val_accuracy: 0.5855 - val_loss: 0.9649\n",
      "Epoch 9/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5187 - loss: 1.0520 - val_accuracy: 0.6085 - val_loss: 0.8907\n",
      "Epoch 10/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5310 - loss: 1.0199 - val_accuracy: 0.6368 - val_loss: 0.8572\n",
      "Epoch 11/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5607 - loss: 0.9791 - val_accuracy: 0.6497 - val_loss: 0.8399\n",
      "Epoch 12/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - accuracy: 0.5766 - loss: 0.9458 - val_accuracy: 0.6425 - val_loss: 0.8079\n",
      "Epoch 13/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - accuracy: 0.5844 - loss: 0.9304 - val_accuracy: 0.6607 - val_loss: 0.7865\n",
      "Epoch 14/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - accuracy: 0.5902 - loss: 0.9148 - val_accuracy: 0.6640 - val_loss: 0.7855\n",
      "Epoch 15/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - accuracy: 0.6071 - loss: 0.9100 - val_accuracy: 0.6826 - val_loss: 0.7606\n",
      "Epoch 16/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - accuracy: 0.6275 - loss: 0.8580 - val_accuracy: 0.6809 - val_loss: 0.7562\n",
      "Epoch 17/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - accuracy: 0.6259 - loss: 0.8391 - val_accuracy: 0.6856 - val_loss: 0.7435\n",
      "Epoch 18/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - accuracy: 0.6396 - loss: 0.8364 - val_accuracy: 0.6643 - val_loss: 0.7392\n",
      "Epoch 19/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - accuracy: 0.6364 - loss: 0.8236 - val_accuracy: 0.6949 - val_loss: 0.7238\n",
      "Epoch 20/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - accuracy: 0.6380 - loss: 0.8186 - val_accuracy: 0.6750 - val_loss: 0.7275\n",
      "Epoch 21/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - accuracy: 0.6489 - loss: 0.7988 - val_accuracy: 0.6910 - val_loss: 0.7132\n",
      "Epoch 22/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - accuracy: 0.6478 - loss: 0.7883 - val_accuracy: 0.6826 - val_loss: 0.6982\n",
      "Epoch 23/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.6629 - loss: 0.7650 - val_accuracy: 0.6985 - val_loss: 0.6986\n",
      "Epoch 24/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - accuracy: 0.6570 - loss: 0.7725 - val_accuracy: 0.7120 - val_loss: 0.6918\n",
      "Epoch 25/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - accuracy: 0.6803 - loss: 0.7531 - val_accuracy: 0.7207 - val_loss: 0.6815\n",
      "Epoch 26/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - accuracy: 0.6716 - loss: 0.7487 - val_accuracy: 0.7131 - val_loss: 0.6743\n",
      "Epoch 27/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - accuracy: 0.6866 - loss: 0.7324 - val_accuracy: 0.7123 - val_loss: 0.6724\n",
      "Epoch 28/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.6792 - loss: 0.7363 - val_accuracy: 0.7047 - val_loss: 0.6748\n",
      "Epoch 29/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - accuracy: 0.6840 - loss: 0.7165 - val_accuracy: 0.7241 - val_loss: 0.6483\n",
      "Epoch 30/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - accuracy: 0.6846 - loss: 0.7367 - val_accuracy: 0.7246 - val_loss: 0.6517\n",
      "Epoch 31/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.6919 - loss: 0.7142 - val_accuracy: 0.7218 - val_loss: 0.6504\n",
      "Epoch 32/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - accuracy: 0.6968 - loss: 0.7012 - val_accuracy: 0.7249 - val_loss: 0.6470\n",
      "Epoch 33/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6966 - loss: 0.6986 - val_accuracy: 0.7221 - val_loss: 0.6477\n",
      "Epoch 34/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7022 - loss: 0.7071 - val_accuracy: 0.7316 - val_loss: 0.6292\n",
      "Epoch 35/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6981 - loss: 0.6980 - val_accuracy: 0.7319 - val_loss: 0.6369\n",
      "Epoch 36/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7094 - loss: 0.6855 - val_accuracy: 0.7294 - val_loss: 0.6299\n",
      "Epoch 37/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - accuracy: 0.7077 - loss: 0.6874 - val_accuracy: 0.7319 - val_loss: 0.6288\n",
      "Epoch 38/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step - accuracy: 0.7176 - loss: 0.6633 - val_accuracy: 0.7375 - val_loss: 0.6322\n",
      "Epoch 39/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - accuracy: 0.7106 - loss: 0.6730 - val_accuracy: 0.7437 - val_loss: 0.6232\n",
      "Epoch 40/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - accuracy: 0.7251 - loss: 0.6384 - val_accuracy: 0.7400 - val_loss: 0.6217\n",
      "Epoch 41/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - accuracy: 0.7139 - loss: 0.6663 - val_accuracy: 0.7386 - val_loss: 0.6203\n",
      "Epoch 42/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7070 - loss: 0.6721 - val_accuracy: 0.7370 - val_loss: 0.6137\n",
      "Epoch 43/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - accuracy: 0.7167 - loss: 0.6634 - val_accuracy: 0.7294 - val_loss: 0.6248\n",
      "Epoch 44/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7076 - loss: 0.6712 - val_accuracy: 0.7389 - val_loss: 0.6205\n",
      "Epoch 45/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - accuracy: 0.7190 - loss: 0.6548 - val_accuracy: 0.7443 - val_loss: 0.6157\n",
      "Epoch 46/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7118 - loss: 0.6625 - val_accuracy: 0.7398 - val_loss: 0.6120\n",
      "Epoch 47/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7201 - loss: 0.6489 - val_accuracy: 0.7476 - val_loss: 0.5987\n",
      "Epoch 48/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7221 - loss: 0.6438 - val_accuracy: 0.7538 - val_loss: 0.6027\n",
      "Epoch 49/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7250 - loss: 0.6444 - val_accuracy: 0.7347 - val_loss: 0.6139\n",
      "Epoch 50/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7192 - loss: 0.6419 - val_accuracy: 0.7400 - val_loss: 0.6028\n",
      "Epoch 51/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7304 - loss: 0.6362 - val_accuracy: 0.7445 - val_loss: 0.6068\n",
      "Epoch 52/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7375 - loss: 0.6178 - val_accuracy: 0.7501 - val_loss: 0.6052\n",
      "Epoch 53/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7349 - loss: 0.6299 - val_accuracy: 0.7510 - val_loss: 0.5969\n",
      "Epoch 54/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7367 - loss: 0.6201 - val_accuracy: 0.7431 - val_loss: 0.6086\n",
      "Epoch 55/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7254 - loss: 0.6378 - val_accuracy: 0.7501 - val_loss: 0.5987\n",
      "Epoch 56/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7360 - loss: 0.6183 - val_accuracy: 0.7532 - val_loss: 0.5929\n",
      "Epoch 57/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - accuracy: 0.7340 - loss: 0.6315 - val_accuracy: 0.7490 - val_loss: 0.5992\n",
      "Epoch 58/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - accuracy: 0.7350 - loss: 0.6363 - val_accuracy: 0.7518 - val_loss: 0.5923\n",
      "Epoch 59/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - accuracy: 0.7435 - loss: 0.6011 - val_accuracy: 0.7428 - val_loss: 0.5967\n",
      "Epoch 60/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - accuracy: 0.7412 - loss: 0.6161 - val_accuracy: 0.7515 - val_loss: 0.5935\n",
      "Epoch 61/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step - accuracy: 0.7418 - loss: 0.6168 - val_accuracy: 0.7471 - val_loss: 0.5994\n",
      "Epoch 62/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - accuracy: 0.7344 - loss: 0.6094 - val_accuracy: 0.7538 - val_loss: 0.5956\n",
      "Epoch 63/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - accuracy: 0.7450 - loss: 0.5931 - val_accuracy: 0.7541 - val_loss: 0.5972\n",
      "Epoch 64/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - accuracy: 0.7512 - loss: 0.5889 - val_accuracy: 0.7583 - val_loss: 0.5962\n",
      "Epoch 65/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step - accuracy: 0.7346 - loss: 0.6072 - val_accuracy: 0.7476 - val_loss: 0.5889\n",
      "Epoch 66/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7384 - loss: 0.6113 - val_accuracy: 0.7476 - val_loss: 0.5903\n",
      "Epoch 67/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7288 - loss: 0.6101 - val_accuracy: 0.7574 - val_loss: 0.5839\n",
      "Epoch 68/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step - accuracy: 0.7304 - loss: 0.6234 - val_accuracy: 0.7465 - val_loss: 0.5929\n",
      "Epoch 69/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - accuracy: 0.7391 - loss: 0.6068 - val_accuracy: 0.7552 - val_loss: 0.5880\n",
      "Epoch 70/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - accuracy: 0.7457 - loss: 0.5918 - val_accuracy: 0.7577 - val_loss: 0.5895\n",
      "Epoch 71/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - accuracy: 0.7442 - loss: 0.5966 - val_accuracy: 0.7583 - val_loss: 0.5859\n",
      "Epoch 72/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7446 - loss: 0.5947 - val_accuracy: 0.7574 - val_loss: 0.5872\n",
      "Epoch 73/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - accuracy: 0.7481 - loss: 0.5789 - val_accuracy: 0.7541 - val_loss: 0.5823\n",
      "Epoch 74/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - accuracy: 0.7458 - loss: 0.5984 - val_accuracy: 0.7543 - val_loss: 0.5789\n",
      "Epoch 75/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.7497 - loss: 0.5967 - val_accuracy: 0.7468 - val_loss: 0.5864\n",
      "Epoch 76/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7547 - loss: 0.5782 - val_accuracy: 0.7546 - val_loss: 0.5824\n",
      "Epoch 77/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7423 - loss: 0.5904 - val_accuracy: 0.7507 - val_loss: 0.5834\n",
      "Epoch 78/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - accuracy: 0.7426 - loss: 0.6043 - val_accuracy: 0.7577 - val_loss: 0.5782\n",
      "Epoch 79/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.7456 - loss: 0.5882 - val_accuracy: 0.7566 - val_loss: 0.5819\n",
      "Epoch 80/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - accuracy: 0.7490 - loss: 0.6006 - val_accuracy: 0.7501 - val_loss: 0.5952\n",
      "Epoch 81/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - accuracy: 0.7461 - loss: 0.5990 - val_accuracy: 0.7681 - val_loss: 0.5714\n",
      "Epoch 82/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - accuracy: 0.7519 - loss: 0.5887 - val_accuracy: 0.7583 - val_loss: 0.5841\n",
      "Epoch 83/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - accuracy: 0.7515 - loss: 0.5841 - val_accuracy: 0.7611 - val_loss: 0.5703\n",
      "Epoch 84/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.7524 - loss: 0.5820 - val_accuracy: 0.7490 - val_loss: 0.5836\n",
      "Epoch 85/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - accuracy: 0.7569 - loss: 0.5745 - val_accuracy: 0.7513 - val_loss: 0.5873\n",
      "Epoch 86/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step - accuracy: 0.7555 - loss: 0.5778 - val_accuracy: 0.7594 - val_loss: 0.5763\n",
      "Epoch 87/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - accuracy: 0.7492 - loss: 0.5823 - val_accuracy: 0.7507 - val_loss: 0.5830\n",
      "Epoch 88/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904us/step - accuracy: 0.7527 - loss: 0.5903 - val_accuracy: 0.7541 - val_loss: 0.5854\n",
      "Epoch 89/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.7522 - loss: 0.5913 - val_accuracy: 0.7619 - val_loss: 0.5726\n",
      "Epoch 90/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - accuracy: 0.7543 - loss: 0.5752 - val_accuracy: 0.7515 - val_loss: 0.5957\n",
      "Epoch 91/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.7423 - loss: 0.5850 - val_accuracy: 0.7591 - val_loss: 0.5755\n",
      "Epoch 92/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - accuracy: 0.7608 - loss: 0.5516 - val_accuracy: 0.7619 - val_loss: 0.5803\n",
      "Epoch 93/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.7402 - loss: 0.5707 - val_accuracy: 0.7611 - val_loss: 0.5774\n",
      "Epoch 94/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.7557 - loss: 0.5705 - val_accuracy: 0.7667 - val_loss: 0.5698\n",
      "Epoch 95/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - accuracy: 0.7513 - loss: 0.5778 - val_accuracy: 0.7616 - val_loss: 0.5791\n",
      "Epoch 96/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - accuracy: 0.7635 - loss: 0.5688 - val_accuracy: 0.7524 - val_loss: 0.5792\n",
      "Epoch 97/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 934us/step - accuracy: 0.7564 - loss: 0.5645 - val_accuracy: 0.7633 - val_loss: 0.5722\n",
      "Epoch 98/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 0.7549 - loss: 0.5625 - val_accuracy: 0.7647 - val_loss: 0.5668\n",
      "Epoch 99/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - accuracy: 0.7613 - loss: 0.5647 - val_accuracy: 0.7647 - val_loss: 0.5752\n",
      "Epoch 100/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - accuracy: 0.7512 - loss: 0.5857 - val_accuracy: 0.7600 - val_loss: 0.5733\n",
      "Epoch 101/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - accuracy: 0.7512 - loss: 0.5688 - val_accuracy: 0.7681 - val_loss: 0.5603\n",
      "Epoch 102/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - accuracy: 0.7619 - loss: 0.5602 - val_accuracy: 0.7689 - val_loss: 0.5641\n",
      "Epoch 103/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - accuracy: 0.7466 - loss: 0.5739 - val_accuracy: 0.7639 - val_loss: 0.5704\n",
      "Epoch 104/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - accuracy: 0.7621 - loss: 0.5547 - val_accuracy: 0.7689 - val_loss: 0.5637\n",
      "Epoch 105/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - accuracy: 0.7569 - loss: 0.5693 - val_accuracy: 0.7566 - val_loss: 0.5677\n",
      "Epoch 106/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7590 - loss: 0.5832 - val_accuracy: 0.7661 - val_loss: 0.5725\n",
      "Epoch 107/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - accuracy: 0.7497 - loss: 0.5804 - val_accuracy: 0.7541 - val_loss: 0.5862\n",
      "Epoch 108/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - accuracy: 0.7619 - loss: 0.5541 - val_accuracy: 0.7628 - val_loss: 0.5710\n",
      "Epoch 109/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.7497 - loss: 0.5673 - val_accuracy: 0.7642 - val_loss: 0.5714\n",
      "Epoch 110/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - accuracy: 0.7582 - loss: 0.5755 - val_accuracy: 0.7698 - val_loss: 0.5632\n",
      "Epoch 111/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.7616 - loss: 0.5703 - val_accuracy: 0.7658 - val_loss: 0.5631\n",
      "Epoch 112/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7614 - loss: 0.5582 - val_accuracy: 0.7644 - val_loss: 0.5660\n",
      "Epoch 113/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - accuracy: 0.7588 - loss: 0.5630 - val_accuracy: 0.7583 - val_loss: 0.5747\n",
      "Epoch 114/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step - accuracy: 0.7642 - loss: 0.5673 - val_accuracy: 0.7642 - val_loss: 0.5656\n",
      "Epoch 115/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7745 - loss: 0.5446 - val_accuracy: 0.7667 - val_loss: 0.5707\n",
      "Epoch 116/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7586 - loss: 0.5685 - val_accuracy: 0.7608 - val_loss: 0.5641\n",
      "Epoch 117/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7619 - loss: 0.5558 - val_accuracy: 0.7594 - val_loss: 0.5690\n",
      "Epoch 118/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7565 - loss: 0.5687 - val_accuracy: 0.7633 - val_loss: 0.5714\n",
      "Epoch 119/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7658 - loss: 0.5519 - val_accuracy: 0.7678 - val_loss: 0.5719\n",
      "Epoch 120/120\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7614 - loss: 0.5643 - val_accuracy: 0.7670 - val_loss: 0.5667\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step\n"
     ]
    }
   ],
   "source": [
    "# Block 5: Model Training and Prediction\n",
    "\n",
    "if len(train_features) > 0:\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(train_features, train_labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(40,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=120, batch_size=32)\n",
    "    \n",
    "    predictions = model.predict(test_features)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    submission = pd.DataFrame({'id': range(len(predicted_labels)), 'category': predicted_labels})\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "else:\n",
    "    print(\"No training features available. Please check the data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
