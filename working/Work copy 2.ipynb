{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Running on CPU.\n",
      "GPU: NVIDIA GeForce RTX 4080 Laptop GPU is available.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Block 1: Importing Libraries\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"GPU is being used.\")\n",
    "else:\n",
    "    print(\"No GPU found. Running on CPU.\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping extraction of train_mp3s.tar.\n",
      "Skipping extraction of test_mp3s.tar.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def extract_tar(tar_file, target_dir):\n",
    "    if os.path.exists(target_dir):\n",
    "        user_input = input(f\"The directory '{target_dir}' already exists. Do you want to skip extraction? (y/n): \")\n",
    "        if user_input.lower() == 'y':\n",
    "            print(f\"Skipping extraction of {tar_file}.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Overwriting the existing directory '{target_dir}'.\")\n",
    "            shutil.rmtree(target_dir)\n",
    "    \n",
    "    with tarfile.open(tar_file, 'r') as tar:\n",
    "        tar.extractall(target_dir)\n",
    "    \n",
    "    # Remove residue \"._\" hidden files\n",
    "    for root, dirs, files in os.walk(target_dir):\n",
    "        for file in files:\n",
    "            if file.startswith(\"._\"):\n",
    "                os.remove(os.path.join(root, file))\n",
    "\n",
    "extract_tar('train_mp3s.tar', 'train_mp3s')\n",
    "extract_tar('test_mp3s.tar', 'test_mp3s')\n",
    "\n",
    "train_labels = np.loadtxt('train_label.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Preprocessing Functions\n",
    "def preprocess_audio(file_path):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        print(f\"Loaded audio file: {file_path}\")\n",
    "        \n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
    "        \n",
    "        # Extract mel-spectrogram features\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        mel_spec_scaled = np.mean(mel_spec_db.T, axis=0)\n",
    "        \n",
    "        # Extract chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "        chroma_scaled = np.mean(chroma.T, axis=0)\n",
    "        \n",
    "        # Extract spectral contrast features\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate)\n",
    "        contrast_scaled = np.mean(contrast.T, axis=0)\n",
    "        \n",
    "        # Extract tonnetz features\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=sample_rate)\n",
    "        tonnetz_scaled = np.mean(tonnetz.T, axis=0)\n",
    "        return mel_spec_scaled\n",
    "        # Concatenate all features\n",
    "        features = np.concatenate((mfccs_scaled, mel_spec_scaled, chroma_scaled, contrast_scaled, tonnetz_scaled))\n",
    "        print(f\"Extracted features: {features.shape}\")\n",
    "        return features\n",
    "        return mel_spec_scaled\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def prepare_data(directory, feature_file):\n",
    "    if os.path.exists(feature_file):\n",
    "        print(f\"Loading features from {feature_file}\")\n",
    "        features = np.load(feature_file)\n",
    "    else:\n",
    "        file_paths = [os.path.join(directory, f\"{i}.mp3\") for i in range(len(os.listdir(directory)))]\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            features = list(executor.map(preprocess_audio, file_paths))\n",
    "\n",
    "        features = [feature for feature in features if feature is not None]\n",
    "\n",
    "        if len(features) == 0:\n",
    "            print(\"No valid features extracted. Please check the data.\")\n",
    "            return None\n",
    "\n",
    "        features = np.array(features)\n",
    "        np.save(feature_file, features)\n",
    "        print(f\"Features saved to {feature_file}\")\n",
    "\n",
    "    print(f\"Processed {len(features)} audio files\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 4: Preparing Data\n",
    "\n",
    "train_feature_file = 'train_features.npy'\n",
    "train_features = prepare_data('train_mp3s/train_mp3s', train_feature_file)\n",
    "\n",
    "if train_features is not None:\n",
    "    print(f\"Train features shape: {train_features.shape}\")\n",
    "else:\n",
    "    print(\"No valid train features. Please check the data.\")\n",
    "\n",
    "test_feature_file = 'test_features.npy'\n",
    "test_features = prepare_data('test_mp3s/test_mp3s', test_feature_file)\n",
    "\n",
    "if test_features is not None:\n",
    "    print(f\"Test features shape: {test_features.shape}\")\n",
    "else:\n",
    "    print(\"No valid test features. Please check the data.\")\n",
    "\n",
    "if train_features is not None and test_features is not None:\n",
    "    train_labels = np.array([int(label) for label in train_labels])\n",
    "    print(f\"Train labels shape: {train_labels.shape}\")\n",
    "\n",
    "    print(f\"Number of training features: {len(train_features)}\")\n",
    "    print(f\"Number of training labels: {len(train_labels)}\")\n",
    "    print(f\"Number of test features: {len(test_features)}\")\n",
    "\n",
    "    # Normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "\n",
    "    # Save the scaler for future use\n",
    "    scaler_filename = 'scaler.pkl'\n",
    "    with open(scaler_filename, 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    print(f\"Scaler saved to {scaler_filename}\")\n",
    "\n",
    "    # Reshape the features for LSTM input\n",
    "    train_features = train_features.reshape((train_features.shape[0], train_features.shape[1], 1))\n",
    "    test_features = test_features.reshape((test_features.shape[0], test_features.shape[1], 1))\n",
    "else:\n",
    "    print(\"Insufficient data. Please check the input files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Block 5: Model Training and Prediction\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_features) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      3\u001b[0m     train_data, val_data, train_labels, val_labels \u001b[38;5;241m=\u001b[39m train_test_split(train_features, train_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      6\u001b[0m         Conv1D(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(train_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m      7\u001b[0m         MaxPooling1D(\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m         Dense(\u001b[38;5;241m4\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m     ])\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Block 5: Model Training and Prediction\n",
    "if len(train_features) > 0:\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(train_features, train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv1D(64, 3, activation='relu', input_shape=(train_features.shape[1], 1)),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(128),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    model.fit(train_data, train_labels, validation_data=(val_data, val_labels),\n",
    "              epochs=30, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "    predictions = model.predict(test_features)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    submission = pd.DataFrame({'id': range(len(predicted_labels)), 'category': predicted_labels})\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "else:\n",
    "    print(\"No training features available. Please check the data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
