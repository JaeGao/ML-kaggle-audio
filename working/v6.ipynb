{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4080 Laptop GPU is available.\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Importing Libraries\n",
    "import random\n",
    "import tarfile\n",
    "import resampy\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import albumentations as A\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping extraction of train_mp3s.tar.\n",
      "Skipping extraction of test_mp3s.tar.\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Extracting Data\n",
    "def extract_tar(tar_file, target_dir):\n",
    "    if os.path.exists(target_dir):\n",
    "        user_input = input(f\"The directory '{target_dir}' already exists. Do you want to skip extraction? (y/n): \")\n",
    "        if user_input.lower() == 'y':\n",
    "            print(f\"Skipping extraction of {tar_file}.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Overwriting the existing directory '{target_dir}'.\")\n",
    "            shutil.rmtree(target_dir)\n",
    "    with tarfile.open(tar_file, 'r') as tar:\n",
    "        tar.extractall(target_dir)\n",
    "    # Remove residue \"._\" hidden files from the inner folder\n",
    "    inner_folder = os.path.join(target_dir, os.path.splitext(os.path.basename(tar_file))[0])\n",
    "    for root, dirs, files in os.walk(inner_folder):\n",
    "        for file in files:\n",
    "            if file.startswith(\"._\"):\n",
    "                os.remove(os.path.join(root, file))\n",
    "\n",
    "extract_tar('train_mp3s.tar', 'train_mp3s')\n",
    "extract_tar('test_mp3s.tar', 'test_mp3s')\n",
    "train_labels = np.loadtxt('train_label.txt', dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Preprocessing Functions\n",
    "def save_preprocessed_data(train_features, train_labels, test_features, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_features.pkl'), 'wb') as f:\n",
    "        pickle.dump(train_features, f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_labels.pkl'), 'wb') as f:\n",
    "        pickle.dump(train_labels, f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'test_features.pkl'), 'wb') as f:\n",
    "        pickle.dump(test_features, f)\n",
    "\n",
    "def load_preprocessed_data(folder_path):\n",
    "    with open(os.path.join(folder_path, 'train_features.pkl'), 'rb') as f:\n",
    "        train_features = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_labels.pkl'), 'rb') as f:\n",
    "        train_labels = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'test_features.pkl'), 'rb') as f:\n",
    "        test_features = pickle.load(f)\n",
    "\n",
    "    return train_features, train_labels, test_features\n",
    "\n",
    "def extract_mfcc(audio, sample_rate):\n",
    "    return librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "\n",
    "def extract_mel_spec(audio, sample_rate):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128)\n",
    "    return mel_spec\n",
    "\n",
    "def extract_tonnetz(audio, sample_rate):\n",
    "    return librosa.feature.tonnetz(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_stft(audio, sample_rate):\n",
    "    return librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_cqt(audio, sample_rate):\n",
    "    return librosa.feature.chroma_cqt(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_cens(audio, sample_rate):\n",
    "    return librosa.feature.chroma_cens(y=audio, sr=sample_rate)\n",
    "\n",
    "def apply_specmix(mel_spec, label, train_features, train_labels, num_mixes=2, alpha=0.2):\n",
    "    # Get the indices of samples with the same label\n",
    "    same_label_indices = np.where(train_labels == label)[0]\n",
    "\n",
    "    if len(same_label_indices) < num_mixes:\n",
    "        # If there are not enough samples with the same label, use all available samples\n",
    "        mix_indices = same_label_indices\n",
    "    else:\n",
    "        # Randomly select num_mixes samples with the same label\n",
    "        mix_indices = np.random.choice(same_label_indices, size=num_mixes, replace=False)\n",
    "\n",
    "    # Get the mel spectrograms of the selected samples\n",
    "    mix_mel_specs = train_features[mix_indices]\n",
    "\n",
    "    # Generate mixing weights using the Beta distribution\n",
    "    weights = np.random.beta(alpha, alpha, size=len(mix_indices))\n",
    "    weights_norm = weights / np.sum(weights)\n",
    "\n",
    "    # Truncate or pad the selected mel spectrograms to match the shape of the input mel spectrogram\n",
    "    target_length = mel_spec.shape[1]\n",
    "    mix_mel_specs_resized = []\n",
    "    for spec in mix_mel_specs:\n",
    "        if len(spec.shape) == 1:\n",
    "            # If spec is 1-dimensional, reshape it to 2-dimensional\n",
    "            spec = spec.reshape(1, -1)\n",
    "        if spec.shape[1] > target_length:\n",
    "            # Truncate the spectrogram if it is longer than the target length\n",
    "            spec = spec[:, :target_length]\n",
    "        elif spec.shape[1] < target_length:\n",
    "            # Pad the spectrogram with zeros if it is shorter than the target length\n",
    "            pad_width = target_length - spec.shape[1]\n",
    "            spec = np.pad(spec, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        mix_mel_specs_resized.append(spec)\n",
    "\n",
    "    # Mix the mel spectrograms using the generated weights\n",
    "    mixed_mel_spec = np.zeros_like(mel_spec)\n",
    "    for i in range(len(mix_indices)):\n",
    "        mixed_mel_spec += weights_norm[i] * mix_mel_specs_resized[i]\n",
    "\n",
    "    return mixed_mel_spec\n",
    "\n",
    "def apply_audio_augmentation(mel_spec, label, train_features, train_labels):\n",
    "    augmented_mel_spec = apply_specmix(mel_spec, label, train_features, train_labels)\n",
    "    return augmented_mel_spec\n",
    "\n",
    "def preprocess_audio(file_path, label, train_features, train_labels, augment=False):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        print(f\"Loaded audio file: {file_path}\")\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128)\n",
    "\n",
    "        if augment and label is not None and train_features is not None and train_labels is not None:\n",
    "            augmented_mel_spec = apply_audio_augmentation(mel_spec, label, train_features, train_labels)\n",
    "            features_scaled = np.mean(augmented_mel_spec.T, axis=0)\n",
    "        else:\n",
    "            features_scaled = np.mean(mel_spec.T, axis=0)\n",
    "\n",
    "        print(f\"Extracted features: {features_scaled.shape}\")\n",
    "        return features_scaled\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_file(file_path, label, train_features, train_labels, augment=False):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    if label is None:\n",
    "        # If label is None, pass None for train_features and train_labels as well\n",
    "        features = preprocess_audio(file_path, None, None, None, augment=augment)\n",
    "    else:\n",
    "        features = preprocess_audio(file_path, label, train_features, train_labels, augment=augment)\n",
    "    return features\n",
    "\n",
    "def prepare_data(directory, train_features, train_labels, augment=False):\n",
    "    file_paths = [os.path.join(directory, f\"{i}.mp3\") for i in range(len(os.listdir(directory)))]\n",
    "    labels = train_labels.tolist() if train_labels is not None else [None] * len(file_paths)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        results = list(executor.map(lambda x: process_file(x[0], x[1], train_features, train_labels, augment), zip(file_paths, labels)))\n",
    "    \n",
    "    features = [feat for feat in results if feat is not None]\n",
    "    \n",
    "    if not features:\n",
    "        raise ValueError(\"No audio files were successfully processed.\")\n",
    "    \n",
    "    features = np.array(features)\n",
    "    print(f\"Processed {len(features)} audio files\")\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data from the 'V6' folder.\n",
      "(2447, 1, 128)\n",
      "(23772, 1, 128)\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Preparing Data (modified)\n",
    "folder_path = 'V6'\n",
    "\n",
    "try:\n",
    "    train_features, train_labels, test_features = load_preprocessed_data(folder_path)\n",
    "    print(\"Loaded preprocessed data from the 'V6' folder.\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "    train_features_original = prepare_data('train_mp3s/train_mp3s', np.empty((0, 128)), train_labels_encoded)\n",
    "    print(f\"Original train features shape: {train_features_original.shape}\")\n",
    "\n",
    "    train_features_augmented = prepare_data('train_mp3s/train_mp3s', train_features_original, train_labels_encoded, augment=True)\n",
    "    print(f\"Augmented train features shape: {train_features_augmented.shape}\")\n",
    "\n",
    "    train_features = np.concatenate((train_features_original, train_features_augmented), axis=0)\n",
    "    print(f\"Combined train features shape: {train_features.shape}\")\n",
    "\n",
    "    test_features = prepare_data('test_mp3s/test_mp3s', np.empty((0, 128)), None)\n",
    "    print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "    train_labels_augmented = train_labels_encoded.copy()\n",
    "    train_labels_encoded = np.concatenate((train_labels_encoded, train_labels_augmented), axis=0)\n",
    "    print(f\"Train labels shape: {train_labels_encoded.shape}\")\n",
    "\n",
    "    print(f\"Number of training features: {len(train_features)}\")\n",
    "    print(f\"Number of training labels: {len(train_labels_encoded)}\")\n",
    "    print(f\"Number of test features: {len(test_features)}\")\n",
    "\n",
    "    save_preprocessed_data(train_features, train_labels_encoded, test_features, folder_path)\n",
    "    print(f\"Saved preprocessed data to the {folder_path} folder.\")\n",
    "\n",
    "if len(train_features) == 0:\n",
    "    print(\"No training features available. Please check the data.\")\n",
    "\n",
    "# Reshape the features to have a channel dimension of 1\n",
    "train_features = train_features.reshape(-1, 1, train_features.shape[1])\n",
    "test_features = test_features.reshape(-1, 1, test_features.shape[1])\n",
    "print(test_features.shape)\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 45.4357, Accuracy: 0.3243, F1 Score: 0.2299, Precision: 0.5297, Recall: 0.3243\n",
      "Epoch [2/10000], Loss: 28.7235, Accuracy: 0.2946, F1 Score: 0.1721, Precision: 0.6092, Recall: 0.2946\n",
      "Epoch [3/10000], Loss: 2.4438, Accuracy: 0.3208, F1 Score: 0.2113, Precision: 0.4654, Recall: 0.3208\n",
      "Epoch [4/10000], Loss: 1.3211, Accuracy: 0.3733, F1 Score: 0.2901, Precision: 0.5476, Recall: 0.3733\n",
      "Epoch [5/10000], Loss: 1.3503, Accuracy: 0.3859, F1 Score: 0.2700, Precision: 0.5348, Recall: 0.3859\n",
      "Epoch [6/10000], Loss: 1.3671, Accuracy: 0.3801, F1 Score: 0.2476, Precision: 0.5193, Recall: 0.3801\n",
      "Epoch [7/10000], Loss: 1.3744, Accuracy: 0.3767, F1 Score: 0.2325, Precision: 0.5304, Recall: 0.3767\n",
      "Epoch [8/10000], Loss: 1.3763, Accuracy: 0.3721, F1 Score: 0.2211, Precision: 0.4668, Recall: 0.3721\n",
      "Epoch [9/10000], Loss: 1.3765, Accuracy: 0.3706, F1 Score: 0.2152, Precision: 0.5163, Recall: 0.3706\n",
      "Epoch [10/10000], Loss: 1.3761, Accuracy: 0.3695, F1 Score: 0.2114, Precision: 0.5211, Recall: 0.3695\n",
      "Epoch [11/10000], Loss: 1.3747, Accuracy: 0.3701, F1 Score: 0.2105, Precision: 0.5773, Recall: 0.3701\n",
      "Epoch [12/10000], Loss: 1.3733, Accuracy: 0.3691, F1 Score: 0.2090, Precision: 0.5381, Recall: 0.3691\n",
      "Epoch [13/10000], Loss: 1.3712, Accuracy: 0.3686, F1 Score: 0.2081, Precision: 0.5363, Recall: 0.3686\n",
      "Epoch [14/10000], Loss: 1.3692, Accuracy: 0.3697, F1 Score: 0.2087, Precision: 0.5412, Recall: 0.3697\n",
      "Epoch [15/10000], Loss: 1.3667, Accuracy: 0.3682, F1 Score: 0.2073, Precision: 0.5152, Recall: 0.3682\n",
      "Epoch [16/10000], Loss: 1.3643, Accuracy: 0.3679, F1 Score: 0.2062, Precision: 0.4905, Recall: 0.3679\n",
      "Epoch [17/10000], Loss: 1.3613, Accuracy: 0.3690, F1 Score: 0.2086, Precision: 0.5397, Recall: 0.3690\n",
      "Epoch [18/10000], Loss: 1.3582, Accuracy: 0.3680, F1 Score: 0.2063, Precision: 0.5539, Recall: 0.3680\n",
      "Epoch [19/10000], Loss: 1.3547, Accuracy: 0.3694, F1 Score: 0.2087, Precision: 0.5548, Recall: 0.3694\n",
      "Epoch [20/10000], Loss: 1.3513, Accuracy: 0.3686, F1 Score: 0.2098, Precision: 0.5164, Recall: 0.3686\n",
      "Epoch [21/10000], Loss: 1.3466, Accuracy: 0.3720, F1 Score: 0.2151, Precision: 0.5630, Recall: 0.3720\n",
      "Epoch [22/10000], Loss: 1.3413, Accuracy: 0.3728, F1 Score: 0.2211, Precision: 0.5502, Recall: 0.3728\n",
      "Epoch [23/10000], Loss: 1.3328, Accuracy: 0.3818, F1 Score: 0.2479, Precision: 0.5565, Recall: 0.3818\n",
      "Epoch [24/10000], Loss: 1.3225, Accuracy: 0.3852, F1 Score: 0.2885, Precision: 0.4531, Recall: 0.3852\n",
      "Epoch [25/10000], Loss: 1.3048, Accuracy: 0.3994, F1 Score: 0.3234, Precision: 0.5120, Recall: 0.3994\n",
      "Epoch [26/10000], Loss: 1.2893, Accuracy: 0.4029, F1 Score: 0.3279, Precision: 0.4921, Recall: 0.4029\n",
      "Epoch [27/10000], Loss: 1.2814, Accuracy: 0.4069, F1 Score: 0.3314, Precision: 0.4437, Recall: 0.4069\n",
      "Epoch [28/10000], Loss: 1.2698, Accuracy: 0.4056, F1 Score: 0.3292, Precision: 0.4594, Recall: 0.4056\n",
      "Epoch [29/10000], Loss: 1.2550, Accuracy: 0.4080, F1 Score: 0.3395, Precision: 0.4601, Recall: 0.4080\n",
      "Epoch [30/10000], Loss: 1.2430, Accuracy: 0.4153, F1 Score: 0.3483, Precision: 0.4700, Recall: 0.4153\n",
      "Epoch [31/10000], Loss: 1.2351, Accuracy: 0.4147, F1 Score: 0.3462, Precision: 0.4886, Recall: 0.4147\n",
      "Epoch [32/10000], Loss: 1.2324, Accuracy: 0.4190, F1 Score: 0.3521, Precision: 0.5023, Recall: 0.4190\n",
      "Epoch [33/10000], Loss: 1.2228, Accuracy: 0.4233, F1 Score: 0.3562, Precision: 0.5048, Recall: 0.4233\n",
      "Epoch [34/10000], Loss: 1.2182, Accuracy: 0.4297, F1 Score: 0.3686, Precision: 0.4995, Recall: 0.4297\n",
      "Epoch [35/10000], Loss: 1.2123, Accuracy: 0.4303, F1 Score: 0.3679, Precision: 0.4870, Recall: 0.4303\n",
      "Epoch [36/10000], Loss: 1.2024, Accuracy: 0.4351, F1 Score: 0.3780, Precision: 0.4907, Recall: 0.4351\n",
      "Epoch [37/10000], Loss: 1.1989, Accuracy: 0.4372, F1 Score: 0.3767, Precision: 0.5147, Recall: 0.4372\n",
      "Epoch [38/10000], Loss: 1.1913, Accuracy: 0.4443, F1 Score: 0.3852, Precision: 0.5148, Recall: 0.4443\n",
      "Epoch [39/10000], Loss: 1.1864, Accuracy: 0.4429, F1 Score: 0.3848, Precision: 0.5209, Recall: 0.4429\n",
      "Epoch [40/10000], Loss: 1.1782, Accuracy: 0.4472, F1 Score: 0.3913, Precision: 0.5115, Recall: 0.4472\n",
      "Epoch [41/10000], Loss: 1.1756, Accuracy: 0.4510, F1 Score: 0.3969, Precision: 0.5235, Recall: 0.4510\n",
      "Epoch [42/10000], Loss: 1.1671, Accuracy: 0.4542, F1 Score: 0.3981, Precision: 0.5337, Recall: 0.4542\n",
      "Epoch [43/10000], Loss: 1.1619, Accuracy: 0.4571, F1 Score: 0.4020, Precision: 0.5360, Recall: 0.4571\n",
      "Epoch [44/10000], Loss: 1.1511, Accuracy: 0.4592, F1 Score: 0.4097, Precision: 0.5255, Recall: 0.4592\n",
      "Epoch [45/10000], Loss: 1.1508, Accuracy: 0.4632, F1 Score: 0.4112, Precision: 0.5442, Recall: 0.4632\n",
      "Epoch [46/10000], Loss: 1.1483, Accuracy: 0.4674, F1 Score: 0.4164, Precision: 0.5466, Recall: 0.4674\n",
      "Epoch [47/10000], Loss: 1.1406, Accuracy: 0.4664, F1 Score: 0.4151, Precision: 0.5420, Recall: 0.4664\n",
      "Epoch [48/10000], Loss: 1.1345, Accuracy: 0.4720, F1 Score: 0.4231, Precision: 0.5565, Recall: 0.4720\n",
      "Epoch [49/10000], Loss: 1.1324, Accuracy: 0.4720, F1 Score: 0.4233, Precision: 0.5491, Recall: 0.4720\n",
      "Epoch [50/10000], Loss: 1.1231, Accuracy: 0.4742, F1 Score: 0.4267, Precision: 0.5500, Recall: 0.4742\n",
      "Epoch [51/10000], Loss: 1.1191, Accuracy: 0.4804, F1 Score: 0.4329, Precision: 0.5590, Recall: 0.4804\n",
      "Epoch [52/10000], Loss: 1.1167, Accuracy: 0.4785, F1 Score: 0.4309, Precision: 0.5618, Recall: 0.4785\n",
      "Epoch [53/10000], Loss: 1.1092, Accuracy: 0.4816, F1 Score: 0.4386, Precision: 0.5524, Recall: 0.4816\n",
      "Epoch [54/10000], Loss: 1.1084, Accuracy: 0.4809, F1 Score: 0.4372, Precision: 0.5511, Recall: 0.4809\n",
      "Epoch [55/10000], Loss: 1.1028, Accuracy: 0.4843, F1 Score: 0.4415, Precision: 0.5562, Recall: 0.4843\n",
      "Epoch [56/10000], Loss: 1.0995, Accuracy: 0.4856, F1 Score: 0.4422, Precision: 0.5631, Recall: 0.4856\n",
      "Epoch [57/10000], Loss: 1.1017, Accuracy: 0.4888, F1 Score: 0.4485, Precision: 0.5632, Recall: 0.4888\n",
      "Epoch [58/10000], Loss: 1.0892, Accuracy: 0.4924, F1 Score: 0.4525, Precision: 0.5732, Recall: 0.4924\n",
      "Epoch [59/10000], Loss: 1.0894, Accuracy: 0.4946, F1 Score: 0.4563, Precision: 0.5785, Recall: 0.4946\n",
      "Epoch [60/10000], Loss: 1.0842, Accuracy: 0.4978, F1 Score: 0.4629, Precision: 0.5776, Recall: 0.4978\n",
      "Epoch [61/10000], Loss: 1.0833, Accuracy: 0.4949, F1 Score: 0.4584, Precision: 0.5754, Recall: 0.4949\n",
      "Epoch [62/10000], Loss: 1.0775, Accuracy: 0.4989, F1 Score: 0.4637, Precision: 0.5815, Recall: 0.4989\n",
      "Epoch [63/10000], Loss: 1.0774, Accuracy: 0.5058, F1 Score: 0.4749, Precision: 0.5842, Recall: 0.5058\n",
      "Epoch [64/10000], Loss: 1.0727, Accuracy: 0.5036, F1 Score: 0.4699, Precision: 0.5872, Recall: 0.5036\n",
      "Epoch [65/10000], Loss: 1.0614, Accuracy: 0.5085, F1 Score: 0.4783, Precision: 0.5904, Recall: 0.5085\n",
      "Epoch [66/10000], Loss: 1.0646, Accuracy: 0.5085, F1 Score: 0.4767, Precision: 0.5942, Recall: 0.5085\n",
      "Epoch [67/10000], Loss: 1.0631, Accuracy: 0.5105, F1 Score: 0.4812, Precision: 0.5927, Recall: 0.5105\n",
      "Epoch [68/10000], Loss: 1.0531, Accuracy: 0.5132, F1 Score: 0.4842, Precision: 0.5949, Recall: 0.5132\n",
      "Epoch [69/10000], Loss: 1.0526, Accuracy: 0.5126, F1 Score: 0.4835, Precision: 0.5941, Recall: 0.5126\n",
      "Epoch [70/10000], Loss: 1.0525, Accuracy: 0.5121, F1 Score: 0.4848, Precision: 0.5924, Recall: 0.5121\n",
      "Epoch [71/10000], Loss: 1.0456, Accuracy: 0.5187, F1 Score: 0.4913, Precision: 0.6017, Recall: 0.5187\n",
      "Epoch [72/10000], Loss: 1.0381, Accuracy: 0.5215, F1 Score: 0.4966, Precision: 0.6015, Recall: 0.5215\n",
      "Epoch [73/10000], Loss: 1.0395, Accuracy: 0.5219, F1 Score: 0.4953, Precision: 0.6068, Recall: 0.5219\n",
      "Epoch [74/10000], Loss: 1.0283, Accuracy: 0.5264, F1 Score: 0.5030, Precision: 0.6041, Recall: 0.5264\n",
      "Epoch [75/10000], Loss: 1.0231, Accuracy: 0.5276, F1 Score: 0.5022, Precision: 0.6163, Recall: 0.5276\n",
      "Epoch [76/10000], Loss: 1.0264, Accuracy: 0.5276, F1 Score: 0.5046, Precision: 0.6095, Recall: 0.5276\n",
      "Epoch [77/10000], Loss: 1.0154, Accuracy: 0.5292, F1 Score: 0.5036, Precision: 0.6187, Recall: 0.5292\n",
      "Epoch [78/10000], Loss: 1.0125, Accuracy: 0.5315, F1 Score: 0.5100, Precision: 0.6148, Recall: 0.5315\n",
      "Epoch [79/10000], Loss: 1.0088, Accuracy: 0.5320, F1 Score: 0.5090, Precision: 0.6203, Recall: 0.5320\n",
      "Epoch [80/10000], Loss: 1.0058, Accuracy: 0.5373, F1 Score: 0.5157, Precision: 0.6256, Recall: 0.5373\n",
      "Epoch [81/10000], Loss: 1.0031, Accuracy: 0.5387, F1 Score: 0.5163, Precision: 0.6294, Recall: 0.5387\n",
      "Epoch [82/10000], Loss: 1.0030, Accuracy: 0.5389, F1 Score: 0.5173, Precision: 0.6284, Recall: 0.5389\n",
      "Epoch [83/10000], Loss: 0.9933, Accuracy: 0.5406, F1 Score: 0.5205, Precision: 0.6304, Recall: 0.5406\n",
      "Epoch [84/10000], Loss: 0.9921, Accuracy: 0.5386, F1 Score: 0.5156, Precision: 0.6317, Recall: 0.5386\n",
      "Epoch [85/10000], Loss: 0.9856, Accuracy: 0.5444, F1 Score: 0.5247, Precision: 0.6350, Recall: 0.5444\n",
      "Epoch [86/10000], Loss: 0.9841, Accuracy: 0.5445, F1 Score: 0.5232, Precision: 0.6384, Recall: 0.5445\n",
      "Epoch [87/10000], Loss: 0.9854, Accuracy: 0.5452, F1 Score: 0.5266, Precision: 0.6311, Recall: 0.5452\n",
      "Epoch [88/10000], Loss: 0.9761, Accuracy: 0.5477, F1 Score: 0.5269, Precision: 0.6457, Recall: 0.5477\n",
      "Epoch [89/10000], Loss: 0.9782, Accuracy: 0.5498, F1 Score: 0.5307, Precision: 0.6413, Recall: 0.5498\n",
      "Epoch [90/10000], Loss: 0.9720, Accuracy: 0.5495, F1 Score: 0.5297, Precision: 0.6442, Recall: 0.5495\n",
      "Epoch [91/10000], Loss: 0.9723, Accuracy: 0.5526, F1 Score: 0.5335, Precision: 0.6443, Recall: 0.5526\n",
      "Epoch [92/10000], Loss: 0.9686, Accuracy: 0.5509, F1 Score: 0.5306, Precision: 0.6466, Recall: 0.5509\n",
      "Epoch [93/10000], Loss: 0.9673, Accuracy: 0.5506, F1 Score: 0.5320, Precision: 0.6421, Recall: 0.5506\n",
      "Epoch [94/10000], Loss: 0.9657, Accuracy: 0.5529, F1 Score: 0.5334, Precision: 0.6523, Recall: 0.5529\n",
      "Epoch [95/10000], Loss: 0.9642, Accuracy: 0.5575, F1 Score: 0.5399, Precision: 0.6517, Recall: 0.5575\n",
      "Epoch [96/10000], Loss: 0.9609, Accuracy: 0.5551, F1 Score: 0.5353, Precision: 0.6563, Recall: 0.5551\n",
      "Epoch [97/10000], Loss: 0.9562, Accuracy: 0.5571, F1 Score: 0.5392, Precision: 0.6515, Recall: 0.5571\n",
      "Epoch [98/10000], Loss: 0.9562, Accuracy: 0.5592, F1 Score: 0.5413, Precision: 0.6594, Recall: 0.5592\n",
      "Epoch [99/10000], Loss: 0.9564, Accuracy: 0.5565, F1 Score: 0.5389, Precision: 0.6500, Recall: 0.5565\n",
      "Epoch [100/10000], Loss: 0.9514, Accuracy: 0.5604, F1 Score: 0.5433, Precision: 0.6533, Recall: 0.5604\n",
      "Epoch [101/10000], Loss: 0.9422, Accuracy: 0.5634, F1 Score: 0.5459, Precision: 0.6664, Recall: 0.5634\n",
      "Epoch [102/10000], Loss: 0.9418, Accuracy: 0.5636, F1 Score: 0.5475, Precision: 0.6567, Recall: 0.5636\n",
      "Epoch [103/10000], Loss: 0.9483, Accuracy: 0.5605, F1 Score: 0.5419, Precision: 0.6599, Recall: 0.5605\n",
      "Epoch [104/10000], Loss: 0.9421, Accuracy: 0.5642, F1 Score: 0.5479, Precision: 0.6578, Recall: 0.5642\n",
      "Epoch [105/10000], Loss: 0.9411, Accuracy: 0.5664, F1 Score: 0.5484, Precision: 0.6688, Recall: 0.5664\n",
      "Epoch [106/10000], Loss: 0.9394, Accuracy: 0.5638, F1 Score: 0.5470, Precision: 0.6592, Recall: 0.5638\n",
      "Epoch [107/10000], Loss: 0.9405, Accuracy: 0.5657, F1 Score: 0.5491, Precision: 0.6618, Recall: 0.5657\n",
      "Epoch [108/10000], Loss: 0.9346, Accuracy: 0.5691, F1 Score: 0.5526, Precision: 0.6673, Recall: 0.5691\n",
      "Epoch [109/10000], Loss: 0.9309, Accuracy: 0.5679, F1 Score: 0.5511, Precision: 0.6672, Recall: 0.5679\n",
      "Epoch [110/10000], Loss: 0.9305, Accuracy: 0.5689, F1 Score: 0.5526, Precision: 0.6656, Recall: 0.5689\n",
      "Epoch [111/10000], Loss: 0.9238, Accuracy: 0.5702, F1 Score: 0.5544, Precision: 0.6675, Recall: 0.5702\n",
      "Epoch [112/10000], Loss: 0.9307, Accuracy: 0.5720, F1 Score: 0.5557, Precision: 0.6712, Recall: 0.5720\n",
      "Epoch [113/10000], Loss: 0.9264, Accuracy: 0.5723, F1 Score: 0.5570, Precision: 0.6654, Recall: 0.5723\n",
      "Epoch [114/10000], Loss: 0.9262, Accuracy: 0.5724, F1 Score: 0.5561, Precision: 0.6707, Recall: 0.5724\n",
      "Epoch [115/10000], Loss: 0.9165, Accuracy: 0.5758, F1 Score: 0.5610, Precision: 0.6713, Recall: 0.5758\n",
      "Epoch [116/10000], Loss: 0.9141, Accuracy: 0.5736, F1 Score: 0.5570, Precision: 0.6768, Recall: 0.5736\n",
      "Epoch [117/10000], Loss: 0.9183, Accuracy: 0.5745, F1 Score: 0.5596, Precision: 0.6696, Recall: 0.5745\n",
      "Epoch [118/10000], Loss: 0.9064, Accuracy: 0.5796, F1 Score: 0.5644, Precision: 0.6846, Recall: 0.5796\n",
      "Epoch [119/10000], Loss: 0.9099, Accuracy: 0.5764, F1 Score: 0.5607, Precision: 0.6756, Recall: 0.5764\n",
      "Epoch [120/10000], Loss: 0.9124, Accuracy: 0.5780, F1 Score: 0.5628, Precision: 0.6806, Recall: 0.5780\n",
      "Epoch [121/10000], Loss: 0.9053, Accuracy: 0.5808, F1 Score: 0.5654, Precision: 0.6838, Recall: 0.5808\n",
      "Epoch [122/10000], Loss: 0.9060, Accuracy: 0.5782, F1 Score: 0.5643, Precision: 0.6751, Recall: 0.5782\n",
      "Epoch [123/10000], Loss: 0.9021, Accuracy: 0.5812, F1 Score: 0.5659, Precision: 0.6833, Recall: 0.5812\n",
      "Epoch [124/10000], Loss: 0.9018, Accuracy: 0.5827, F1 Score: 0.5685, Precision: 0.6836, Recall: 0.5827\n",
      "Epoch [125/10000], Loss: 0.9001, Accuracy: 0.5824, F1 Score: 0.5683, Precision: 0.6853, Recall: 0.5824\n",
      "Epoch [126/10000], Loss: 0.9000, Accuracy: 0.5843, F1 Score: 0.5696, Precision: 0.6868, Recall: 0.5843\n",
      "Epoch [127/10000], Loss: 0.9020, Accuracy: 0.5852, F1 Score: 0.5706, Precision: 0.6857, Recall: 0.5852\n",
      "Epoch [128/10000], Loss: 0.8944, Accuracy: 0.5861, F1 Score: 0.5717, Precision: 0.6879, Recall: 0.5861\n",
      "Epoch [129/10000], Loss: 0.8928, Accuracy: 0.5854, F1 Score: 0.5712, Precision: 0.6889, Recall: 0.5854\n",
      "Epoch [130/10000], Loss: 0.8859, Accuracy: 0.5882, F1 Score: 0.5733, Precision: 0.6910, Recall: 0.5882\n",
      "Epoch [131/10000], Loss: 0.8901, Accuracy: 0.5871, F1 Score: 0.5732, Precision: 0.6864, Recall: 0.5871\n",
      "Epoch [132/10000], Loss: 0.8869, Accuracy: 0.5890, F1 Score: 0.5743, Precision: 0.6977, Recall: 0.5890\n",
      "Epoch [133/10000], Loss: 0.8806, Accuracy: 0.5918, F1 Score: 0.5788, Precision: 0.6937, Recall: 0.5918\n",
      "Epoch [134/10000], Loss: 0.8875, Accuracy: 0.5886, F1 Score: 0.5748, Precision: 0.6909, Recall: 0.5886\n",
      "Epoch [135/10000], Loss: 0.8841, Accuracy: 0.5937, F1 Score: 0.5803, Precision: 0.6947, Recall: 0.5937\n",
      "Epoch [136/10000], Loss: 0.8839, Accuracy: 0.5935, F1 Score: 0.5800, Precision: 0.6847, Recall: 0.5935\n",
      "Epoch [137/10000], Loss: 0.8844, Accuracy: 0.5928, F1 Score: 0.5800, Precision: 0.6934, Recall: 0.5928\n",
      "Epoch [138/10000], Loss: 0.8849, Accuracy: 0.5931, F1 Score: 0.5793, Precision: 0.6944, Recall: 0.5931\n",
      "Epoch [139/10000], Loss: 0.8811, Accuracy: 0.5956, F1 Score: 0.5830, Precision: 0.6943, Recall: 0.5956\n",
      "Epoch [140/10000], Loss: 0.8728, Accuracy: 0.5941, F1 Score: 0.5801, Precision: 0.6979, Recall: 0.5941\n",
      "Epoch [141/10000], Loss: 0.8749, Accuracy: 0.5949, F1 Score: 0.5827, Precision: 0.6891, Recall: 0.5949\n",
      "Epoch [142/10000], Loss: 0.8824, Accuracy: 0.5940, F1 Score: 0.5795, Precision: 0.6955, Recall: 0.5940\n",
      "Epoch [143/10000], Loss: 0.8735, Accuracy: 0.5942, F1 Score: 0.5824, Precision: 0.6848, Recall: 0.5942\n",
      "Epoch [144/10000], Loss: 0.8675, Accuracy: 0.6002, F1 Score: 0.5864, Precision: 0.7034, Recall: 0.6002\n",
      "Epoch [145/10000], Loss: 0.8686, Accuracy: 0.5996, F1 Score: 0.5874, Precision: 0.6903, Recall: 0.5996\n",
      "Epoch [146/10000], Loss: 0.8633, Accuracy: 0.5982, F1 Score: 0.5849, Precision: 0.6936, Recall: 0.5982\n",
      "Epoch [147/10000], Loss: 0.8649, Accuracy: 0.6030, F1 Score: 0.5909, Precision: 0.6929, Recall: 0.6030\n",
      "Epoch [148/10000], Loss: 0.8608, Accuracy: 0.6024, F1 Score: 0.5902, Precision: 0.6940, Recall: 0.6024\n",
      "Epoch [149/10000], Loss: 0.8640, Accuracy: 0.5998, F1 Score: 0.5877, Precision: 0.6898, Recall: 0.5998\n",
      "Epoch [150/10000], Loss: 0.8635, Accuracy: 0.6023, F1 Score: 0.5901, Precision: 0.6915, Recall: 0.6023\n",
      "Epoch [151/10000], Loss: 0.8638, Accuracy: 0.6031, F1 Score: 0.5911, Precision: 0.6907, Recall: 0.6031\n",
      "Epoch [152/10000], Loss: 0.8598, Accuracy: 0.6035, F1 Score: 0.5909, Precision: 0.6903, Recall: 0.6035\n",
      "Epoch [153/10000], Loss: 0.8528, Accuracy: 0.6048, F1 Score: 0.5930, Precision: 0.6865, Recall: 0.6048\n",
      "Epoch [154/10000], Loss: 0.8527, Accuracy: 0.6062, F1 Score: 0.5947, Precision: 0.6890, Recall: 0.6062\n",
      "Epoch [155/10000], Loss: 0.8516, Accuracy: 0.6065, F1 Score: 0.5950, Precision: 0.6826, Recall: 0.6065\n",
      "Epoch [156/10000], Loss: 0.8512, Accuracy: 0.6086, F1 Score: 0.5970, Precision: 0.6957, Recall: 0.6086\n",
      "Epoch [157/10000], Loss: 0.8548, Accuracy: 0.6111, F1 Score: 0.6013, Precision: 0.6801, Recall: 0.6111\n",
      "Epoch [158/10000], Loss: 0.8514, Accuracy: 0.6059, F1 Score: 0.5944, Precision: 0.6866, Recall: 0.6059\n",
      "Epoch [159/10000], Loss: 0.8526, Accuracy: 0.6116, F1 Score: 0.6006, Precision: 0.6918, Recall: 0.6116\n",
      "Epoch [160/10000], Loss: 0.8470, Accuracy: 0.6078, F1 Score: 0.5965, Precision: 0.6890, Recall: 0.6078\n",
      "Epoch [161/10000], Loss: 0.8462, Accuracy: 0.6122, F1 Score: 0.6008, Precision: 0.6972, Recall: 0.6122\n",
      "Epoch [162/10000], Loss: 0.8417, Accuracy: 0.6133, F1 Score: 0.6031, Precision: 0.6886, Recall: 0.6133\n",
      "Epoch [163/10000], Loss: 0.8450, Accuracy: 0.6128, F1 Score: 0.6015, Precision: 0.6912, Recall: 0.6128\n",
      "Epoch [164/10000], Loss: 0.8427, Accuracy: 0.6149, F1 Score: 0.6047, Precision: 0.6898, Recall: 0.6149\n",
      "Epoch [165/10000], Loss: 0.8433, Accuracy: 0.6140, F1 Score: 0.6030, Precision: 0.6931, Recall: 0.6140\n",
      "Epoch [166/10000], Loss: 0.8397, Accuracy: 0.6153, F1 Score: 0.6046, Precision: 0.6914, Recall: 0.6153\n",
      "Epoch [167/10000], Loss: 0.8383, Accuracy: 0.6154, F1 Score: 0.6049, Precision: 0.6912, Recall: 0.6154\n",
      "Epoch [168/10000], Loss: 0.8340, Accuracy: 0.6172, F1 Score: 0.6066, Precision: 0.6913, Recall: 0.6172\n",
      "Epoch [169/10000], Loss: 0.8346, Accuracy: 0.6165, F1 Score: 0.6063, Precision: 0.6924, Recall: 0.6165\n",
      "Epoch [170/10000], Loss: 0.8355, Accuracy: 0.6148, F1 Score: 0.6043, Precision: 0.6954, Recall: 0.6148\n",
      "Epoch [171/10000], Loss: 0.8358, Accuracy: 0.6142, F1 Score: 0.6035, Precision: 0.6927, Recall: 0.6142\n",
      "Epoch [172/10000], Loss: 0.8376, Accuracy: 0.6150, F1 Score: 0.6044, Precision: 0.6891, Recall: 0.6150\n",
      "Epoch [173/10000], Loss: 0.8339, Accuracy: 0.6168, F1 Score: 0.6072, Precision: 0.6821, Recall: 0.6168\n",
      "Epoch [174/10000], Loss: 0.8311, Accuracy: 0.6176, F1 Score: 0.6076, Precision: 0.6871, Recall: 0.6176\n",
      "Epoch [175/10000], Loss: 0.8275, Accuracy: 0.6183, F1 Score: 0.6084, Precision: 0.6856, Recall: 0.6183\n",
      "Epoch [176/10000], Loss: 0.8268, Accuracy: 0.6184, F1 Score: 0.6080, Precision: 0.6856, Recall: 0.6184\n",
      "Epoch [177/10000], Loss: 0.8265, Accuracy: 0.6195, F1 Score: 0.6091, Precision: 0.6902, Recall: 0.6195\n",
      "Epoch [178/10000], Loss: 0.8264, Accuracy: 0.6208, F1 Score: 0.6106, Precision: 0.6929, Recall: 0.6208\n",
      "Epoch [179/10000], Loss: 0.8269, Accuracy: 0.6197, F1 Score: 0.6094, Precision: 0.6865, Recall: 0.6197\n",
      "Epoch [180/10000], Loss: 0.8208, Accuracy: 0.6193, F1 Score: 0.6094, Precision: 0.6846, Recall: 0.6193\n",
      "Epoch [181/10000], Loss: 0.8228, Accuracy: 0.6204, F1 Score: 0.6099, Precision: 0.6927, Recall: 0.6204\n",
      "Epoch [182/10000], Loss: 0.8217, Accuracy: 0.6227, F1 Score: 0.6135, Precision: 0.6948, Recall: 0.6227\n",
      "Epoch [183/10000], Loss: 0.8235, Accuracy: 0.6197, F1 Score: 0.6093, Precision: 0.6974, Recall: 0.6197\n",
      "Epoch [184/10000], Loss: 0.8218, Accuracy: 0.6226, F1 Score: 0.6134, Precision: 0.6971, Recall: 0.6226\n",
      "Epoch [185/10000], Loss: 0.8146, Accuracy: 0.6229, F1 Score: 0.6129, Precision: 0.6938, Recall: 0.6229\n",
      "Epoch [186/10000], Loss: 0.8240, Accuracy: 0.6241, F1 Score: 0.6147, Precision: 0.6879, Recall: 0.6241\n",
      "Epoch [187/10000], Loss: 0.8138, Accuracy: 0.6245, F1 Score: 0.6145, Precision: 0.6921, Recall: 0.6245\n",
      "Epoch [188/10000], Loss: 0.8162, Accuracy: 0.6236, F1 Score: 0.6147, Precision: 0.6856, Recall: 0.6236\n",
      "Epoch [189/10000], Loss: 0.8170, Accuracy: 0.6223, F1 Score: 0.6123, Precision: 0.6869, Recall: 0.6223\n",
      "Epoch [190/10000], Loss: 0.8164, Accuracy: 0.6216, F1 Score: 0.6113, Precision: 0.6927, Recall: 0.6216\n",
      "Epoch [191/10000], Loss: 0.8115, Accuracy: 0.6263, F1 Score: 0.6167, Precision: 0.6938, Recall: 0.6263\n",
      "Epoch [192/10000], Loss: 0.8090, Accuracy: 0.6260, F1 Score: 0.6166, Precision: 0.6880, Recall: 0.6260\n",
      "Epoch [193/10000], Loss: 0.8102, Accuracy: 0.6278, F1 Score: 0.6188, Precision: 0.6900, Recall: 0.6278\n",
      "Epoch [194/10000], Loss: 0.8084, Accuracy: 0.6297, F1 Score: 0.6209, Precision: 0.6969, Recall: 0.6297\n",
      "Epoch [195/10000], Loss: 0.8150, Accuracy: 0.6275, F1 Score: 0.6178, Precision: 0.6969, Recall: 0.6275\n",
      "Epoch [196/10000], Loss: 0.8105, Accuracy: 0.6287, F1 Score: 0.6193, Precision: 0.6921, Recall: 0.6287\n",
      "Epoch [197/10000], Loss: 0.8034, Accuracy: 0.6318, F1 Score: 0.6230, Precision: 0.7017, Recall: 0.6318\n",
      "Epoch [198/10000], Loss: 0.8066, Accuracy: 0.6259, F1 Score: 0.6160, Precision: 0.6984, Recall: 0.6259\n",
      "Epoch [199/10000], Loss: 0.7995, Accuracy: 0.6312, F1 Score: 0.6226, Precision: 0.6986, Recall: 0.6312\n"
     ]
    }
   ],
   "source": [
    "# Block 5: Model Definition, Training, and Prediction\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * input_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        running_f1 = 0.0\n",
    "        running_precision = 0.0\n",
    "        running_recall = 0.0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            running_accuracy += accuracy_score(labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "            running_f1 += f1_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='weighted', zero_division=1)\n",
    "            running_precision += precision_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='weighted', zero_division=1)\n",
    "            running_recall += recall_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='weighted', zero_division=1)\n",
    "        \n",
    "        scheduler.step()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = running_accuracy / len(train_loader)\n",
    "        epoch_f1 = running_f1 / len(train_loader)\n",
    "        epoch_precision = running_precision / len(train_loader)\n",
    "        epoch_recall = running_recall / len(train_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, F1 Score: {epoch_f1:.4f}, Precision: {epoch_precision:.4f}, Recall: {epoch_recall:.4f}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = AudioDataset(train_features, train_labels_encoded)\n",
    "test_dataset = AudioDataset(test_features, np.zeros(len(test_features)))  # Dummy labels for test data\n",
    "train_loader = DataLoader(train_dataset, batch_size=8192, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8192)\n",
    "\n",
    "# Model parameters\n",
    "input_size = train_features.shape[2]  # Assuming the input features have shape (batch_size, 1, feature_size)\n",
    "hidden_size = 2048\n",
    "num_classes = len(np.unique(train_labels_encoded))\n",
    "num_epochs = 10000\n",
    "\n",
    "# Create the model\n",
    "model = AudioClassifier(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, scheduler, device, num_epochs)\n",
    "\n",
    "# Generate submission.csv\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for features, _ in test_loader:\n",
    "        features = features.to(device)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "submission = pd.DataFrame({'id': range(len(predictions)), 'category': predictions})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
