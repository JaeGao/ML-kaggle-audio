{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4080 Laptop GPU is available.\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Importing Libraries\n",
    "import tarfile\n",
    "import resampy\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import albumentations as A\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping extraction of train_mp3s.tar.\n",
      "Skipping extraction of test_mp3s.tar.\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Extracting Data\n",
    "def extract_tar(tar_file, target_dir):\n",
    "    if os.path.exists(target_dir):\n",
    "        user_input = input(f\"The directory '{target_dir}' already exists. Do you want to skip extraction? (y/n): \")\n",
    "        if user_input.lower() == 'y':\n",
    "            print(f\"Skipping extraction of {tar_file}.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Overwriting the existing directory '{target_dir}'.\")\n",
    "            shutil.rmtree(target_dir)\n",
    "    with tarfile.open(tar_file, 'r') as tar:\n",
    "        tar.extractall(target_dir)\n",
    "    # Remove residue \"._\" hidden files from the inner folder\n",
    "    inner_folder = os.path.join(target_dir, os.path.splitext(os.path.basename(tar_file))[0])\n",
    "    for root, dirs, files in os.walk(inner_folder):\n",
    "        for file in files:\n",
    "            if file.startswith(\"._\"):\n",
    "                os.remove(os.path.join(root, file))\n",
    "\n",
    "extract_tar('train_mp3s.tar', 'train_mp3s')\n",
    "extract_tar('test_mp3s.tar', 'test_mp3s')\n",
    "train_labels = np.loadtxt('train_label.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Preprocessing Functions\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_preprocessed_data(data, labels, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_features.pkl'), 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_labels.pkl'), 'wb') as f:\n",
    "        pickle.dump(labels, f)\n",
    "\n",
    "def load_preprocessed_data(folder_path):\n",
    "    with open(os.path.join(folder_path, 'train_features.pkl'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_labels.pkl'), 'rb') as f:\n",
    "        labels = pickle.load(f)\n",
    "\n",
    "    return data, labels\n",
    "import concurrent.futures\n",
    "\n",
    "def extract_mfcc(audio, sample_rate):\n",
    "    return librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "\n",
    "def extract_spectral_contrast(audio, sample_rate):\n",
    "    return librosa.feature.spectral_contrast(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_tonnetz(audio, sample_rate):\n",
    "    return librosa.feature.tonnetz(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_stft(audio, sample_rate):\n",
    "    return librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_cqt(audio, sample_rate):\n",
    "    return librosa.feature.chroma_cqt(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_cens(audio, sample_rate):\n",
    "    return librosa.feature.chroma_cens(y=audio, sr=sample_rate)\n",
    "\n",
    "def preprocess_audio(file_path, augment=False):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        print(f\"Loaded audio file: {file_path}\")\n",
    "\n",
    "        # Apply audio augmentation\n",
    "        if augment:\n",
    "            audio = apply_audio_augmentation(audio, sample_rate)\n",
    "\n",
    "        # Extract features in parallel\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future_mfcc = executor.submit(extract_mfcc, audio, sample_rate)\n",
    "            future_spectral_contrast = executor.submit(extract_spectral_contrast, audio, sample_rate)\n",
    "            future_tonnetz = executor.submit(extract_tonnetz, audio, sample_rate)\n",
    "            future_chroma_stft = executor.submit(extract_chroma_stft, audio, sample_rate)\n",
    "            future_chroma_cqt = executor.submit(extract_chroma_cqt, audio, sample_rate)\n",
    "            future_chroma_cens = executor.submit(extract_chroma_cens, audio, sample_rate)\n",
    "\n",
    "            mfccs = future_mfcc.result()\n",
    "            spectral_contrast = future_spectral_contrast.result()\n",
    "            tonnetz = future_tonnetz.result()\n",
    "            chroma_stft = future_chroma_stft.result()\n",
    "            chroma_cqt = future_chroma_cqt.result()\n",
    "            chroma_cens = future_chroma_cens.result()\n",
    "\n",
    "        # Concatenate features\n",
    "        features = np.concatenate((mfccs, spectral_contrast, tonnetz, chroma_stft, chroma_cqt, chroma_cens), axis=0)\n",
    "\n",
    "        features_scaled = np.mean(features.T, axis=0)\n",
    "        print(f\"Extracted features: {features_scaled.shape}\")\n",
    "        return features_scaled\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "def apply_audio_augmentation(audio, sample_rate):\n",
    "    # Apply pitch shifting\n",
    "    pitch_shift = np.random.randint(-2, 2)\n",
    "    audio = librosa.effects.pitch_shift(audio, sr=sample_rate, n_steps=pitch_shift)\n",
    "\n",
    "    # Apply time stretching\n",
    "    stretch_factor = np.random.uniform(0.8, 1.2)\n",
    "    audio = librosa.effects.time_stretch(audio, rate=stretch_factor)\n",
    "\n",
    "    # Apply noise injection\n",
    "    noise_amp = 0.005 * np.random.uniform(0, 1)\n",
    "    noise = noise_amp * np.random.normal(size=audio.shape[0])\n",
    "    audio = audio + noise\n",
    "\n",
    "    return audio\n",
    "\n",
    "def process_file(file_path, augment=False):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    features = preprocess_audio(file_path, augment=augment)\n",
    "    return features\n",
    "\n",
    "def prepare_data(directory, augment=False):\n",
    "    file_paths = [os.path.join(directory, f\"{i}.mp3\") for i in range(len(os.listdir(directory)))]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(lambda x: process_file(x, augment), file_paths))\n",
    "    features = [feat for feat in results if feat is not None]\n",
    "    print(f\"Processed {len(features)} audio files\")\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: test_mp3s/test_mp3s\\0.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\1.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\2.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\0.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\1.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\2.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\3.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\4.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\3.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\4.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\5.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\6.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\5.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\0.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\1.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\6.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\7.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\2.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\3.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\4.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\7.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\8.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\5.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\8.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\9.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\10.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\11.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\12.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\9.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\6.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\7.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\10.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\11.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\12.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\13.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\14.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\15.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\16.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\17.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\18.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\19.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\20.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\21.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\22.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\23.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\8.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\9.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\10.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\11.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\12.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\13.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\14.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\15.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\16.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\17.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\18.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\19.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\20.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\21.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\22.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\23.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\24.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\25.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\26.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\27.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\28.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\29.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\30.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\31.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\13.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\14.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\15.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\16.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\17.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\18.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\19.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\20.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\21.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\22.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\23.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\24.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\24.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\25.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\25.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\26.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\27.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\28.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\29.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\30.mp3\n",
      "Processing file: train_mp3s/train_mp3s\\31.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\26.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\27.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\28.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\29.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\30.mp3\n",
      "Processing file: test_mp3s/test_mp3s\\31.mp3\n",
      "Loaded audio file: test_mp3s/test_mp3s\\13.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\30.mp3\n",
      "Loaded audio file: test_mp3s/test_mp3s\\6.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\12.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\19.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\6.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\10.mp3\n",
      "Loaded audio file: test_mp3s/test_mp3s\\11.mp3\n",
      "Loaded audio file: train_mp3s/train_mp3s\\28.mp3\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Block 4: Preparing Data (modified)\n",
    "folder_path = 'V5'\n",
    "\n",
    "try:\n",
    "    train_features, train_labels = load_preprocessed_data(folder_path)\n",
    "    print(\"Loaded preprocessed data from the 'V5' folder.\")\n",
    "except FileNotFoundError:\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_train_features_original = executor.submit(prepare_data, 'train_mp3s/train_mp3s')\n",
    "        future_train_features_augmented = executor.submit(prepare_data, 'train_mp3s/train_mp3s', augment=True)\n",
    "        future_test_features = executor.submit(prepare_data, 'test_mp3s/test_mp3s')\n",
    "\n",
    "        train_features_original = future_train_features_original.result()\n",
    "        print(f\"Original train features shape: {train_features_original.shape}\")\n",
    "\n",
    "        train_features_augmented = future_train_features_augmented.result()\n",
    "        print(f\"Augmented train features shape: {train_features_augmented.shape}\")\n",
    "\n",
    "        train_features = np.concatenate((train_features_original, train_features_augmented), axis=0)\n",
    "        print(f\"Combined train features shape: {train_features.shape}\")\n",
    "\n",
    "        test_features = future_test_features.result()\n",
    "        print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_original = label_encoder.fit_transform(train_labels)\n",
    "    train_labels_augmented = train_labels_original.copy()\n",
    "    train_labels = np.concatenate((train_labels_original, train_labels_augmented), axis=0)\n",
    "    print(f\"Train labels shape: {train_labels.shape}\")\n",
    "\n",
    "    print(f\"Number of training features: {len(train_features)}\")\n",
    "    print(f\"Number of training labels: {len(train_labels)}\")\n",
    "    print(f\"Number of test features: {len(test_features)}\")\n",
    "\n",
    "    save_preprocessed_data(train_features, train_labels, folder_path)\n",
    "    print(\"Saved preprocessed data to the 'V5' folder.\")\n",
    "\n",
    "if len(train_features) == 0:\n",
    "    print(\"No training features available. Please check the data.\")\n",
    "\n",
    "# Block 4: Preparing Data (modified)\n",
    "folder_path = 'V5'\n",
    "\n",
    "try:\n",
    "    train_features, train_labels = load_preprocessed_data(folder_path)\n",
    "    print(\"Loaded preprocessed data from the 'V5' folder.\")\n",
    "except FileNotFoundError:\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_train_features_original = executor.submit(prepare_data, 'train_mp3s/train_mp3s')\n",
    "        future_train_features_augmented = executor.submit(prepare_data, 'train_mp3s/train_mp3s', augment=True)\n",
    "        future_test_features = executor.submit(prepare_data, 'test_mp3s/test_mp3s')\n",
    "\n",
    "        train_features_original = future_train_features_original.result()\n",
    "        print(f\"Original train features shape: {train_features_original.shape}\")\n",
    "\n",
    "        train_features_augmented = future_train_features_augmented.result()\n",
    "        print(f\"Augmented train features shape: {train_features_augmented.shape}\")\n",
    "\n",
    "        train_features = np.concatenate((train_features_original, train_features_augmented), axis=0)\n",
    "        print(f\"Combined train features shape: {train_features.shape}\")\n",
    "\n",
    "        test_features = future_test_features.result()\n",
    "        print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_original = label_encoder.fit_transform(train_labels)\n",
    "    train_labels_augmented = train_labels_original.copy()\n",
    "    train_labels = np.concatenate((train_labels_original, train_labels_augmented), axis=0)\n",
    "    print(f\"Train labels shape: {train_labels.shape}\")\n",
    "\n",
    "    print(f\"Number of training features: {len(train_features)}\")\n",
    "    print(f\"Number of training labels: {len(train_labels)}\")\n",
    "    print(f\"Number of test features: {len(test_features)}\")\n",
    "\n",
    "    save_preprocessed_data(train_features, train_labels, folder_path)\n",
    "    print(\"Saved preprocessed data to the 'V5' folder.\")\n",
    "\n",
    "if len(train_features) == 0:\n",
    "    print(\"No training features available. Please check the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5: Model Training and Prediction\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if len(train_features) > 0:\n",
    "    if len(train_features) != len(train_labels):\n",
    "        raise ValueError(\"Number of train features and labels do not match.\")\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "\n",
    "    # Create a custom dataset class\n",
    "    class AudioDataset(Dataset):\n",
    "        def __init__(self, features, labels):\n",
    "            self.features = torch.tensor(features, dtype=torch.float32)\n",
    "            self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.features)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.features[index], self.labels[index]\n",
    "\n",
    "    # Set up cross-validation\n",
    "    num_folds = 10\n",
    "    kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize variables to store the best model and its metrics\n",
    "    best_model = None\n",
    "    best_accuracy = 0.0\n",
    "    best_f1 = 0.0\n",
    "    best_precision = 0.0\n",
    "    best_recall = 0.0\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(train_features, train_labels), 1):\n",
    "        print(f\"Fold {fold}/{num_folds}\")\n",
    "\n",
    "        # Create data subsets for the current fold\n",
    "        train_data = train_features[train_indices]\n",
    "        train_labels_fold = train_labels[train_indices]\n",
    "        val_data = train_features[val_indices]\n",
    "        val_labels_fold = train_labels[val_indices]\n",
    "\n",
    "        # Create PyTorch datasets\n",
    "        train_dataset = AudioDataset(train_data, train_labels_fold)\n",
    "        val_dataset = AudioDataset(val_data, val_labels_fold)\n",
    "\n",
    "        # Create data loaders\n",
    "        batch_size = 64\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Define the model architecture\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(train_features.shape[1], 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 4)\n",
    "        ).to(device)\n",
    "\n",
    "        # Define the loss function, optimizer, and scheduler\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "\n",
    "        # Train the model\n",
    "        num_epochs = 200\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_data, batch_labels in train_loader:\n",
    "                batch_data = batch_data.to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                predictions = []\n",
    "                true_labels = []\n",
    "                for batch_data, batch_labels in val_loader:\n",
    "                    batch_data = batch_data.to(device)\n",
    "                    batch_labels = batch_labels.to(device)\n",
    "\n",
    "                    outputs = model(batch_data)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    predictions.extend(predicted.cpu().numpy())\n",
    "                    true_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "                accuracy = accuracy_score(true_labels, predictions)\n",
    "                f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "                precision = precision_score(true_labels, predictions, average='weighted')\n",
    "                recall = recall_score(true_labels, predictions, average='weighted')\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        # Update the best model if the current model has higher F1 score\n",
    "        if f1 > best_f1:\n",
    "            best_model = model\n",
    "            best_accuracy = accuracy\n",
    "            best_f1 = f1\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    test_features = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(test_features)\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        predicted_labels = label_encoder.inverse_transform(predicted_labels.cpu().numpy())\n",
    "\n",
    "    submission = pd.DataFrame({'id': range(len(predicted_labels)), 'category': predicted_labels})\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "    print(f\"Best model metrics:\")\n",
    "    print(f\"Accuracy: {best_accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"Precision: {best_precision:.4f}\")\n",
    "    print(f\"Recall: {best_recall:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No training features available. Please check the data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
