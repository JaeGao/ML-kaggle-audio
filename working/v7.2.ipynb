{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4080 Laptop GPU is available.\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Importing Libraries\n",
    "import random\n",
    "import tarfile\n",
    "import resampy\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import albumentations as A\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping extraction of train_mp3s.tar.\n",
      "Skipping extraction of test_mp3s.tar.\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Extracting Data\n",
    "def extract_tar(tar_file, target_dir):\n",
    "    if os.path.exists(target_dir):\n",
    "        user_input = input(f\"The directory '{target_dir}' already exists. Do you want to skip extraction? (y/n): \")\n",
    "        if user_input.lower() == 'y':\n",
    "            print(f\"Skipping extraction of {tar_file}.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Overwriting the existing directory '{target_dir}'.\")\n",
    "            shutil.rmtree(target_dir)\n",
    "    with tarfile.open(tar_file, 'r') as tar:\n",
    "        tar.extractall(target_dir)\n",
    "    # Remove residue \"._\" hidden files from the inner folder\n",
    "    inner_folder = os.path.join(target_dir, os.path.splitext(os.path.basename(tar_file))[0])\n",
    "    for root, dirs, files in os.walk(inner_folder):\n",
    "        for file in files:\n",
    "            if file.startswith(\"._\"):\n",
    "                os.remove(os.path.join(root, file))\n",
    "\n",
    "extract_tar('train_mp3s.tar', 'train_mp3s')\n",
    "extract_tar('test_mp3s.tar', 'test_mp3s')\n",
    "train_labels = np.loadtxt('train_label.txt', dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Preprocessing Functions\n",
    "def save_preprocessed_data(train_features, train_labels, test_features, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_features.pkl'), 'wb') as f:\n",
    "        pickle.dump(train_features, f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_labels.pkl'), 'wb') as f:\n",
    "        pickle.dump(train_labels, f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'test_features.pkl'), 'wb') as f:\n",
    "        pickle.dump(test_features, f)\n",
    "\n",
    "def load_preprocessed_data(folder_path):\n",
    "    with open(os.path.join(folder_path, 'train_features.pkl'), 'rb') as f:\n",
    "        train_features = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_labels.pkl'), 'rb') as f:\n",
    "        train_labels = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'test_features.pkl'), 'rb') as f:\n",
    "        test_features = pickle.load(f)\n",
    "\n",
    "    return train_features, train_labels, test_features\n",
    "\n",
    "def extract_mfcc(audio, sample_rate):\n",
    "    return librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "\n",
    "def extract_mel_spec(audio, sample_rate):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128)\n",
    "    return mel_spec\n",
    "\n",
    "def extract_tonnetz(audio, sample_rate):\n",
    "    return librosa.feature.tonnetz(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_stft(audio, sample_rate):\n",
    "    return librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_cqt(audio, sample_rate):\n",
    "    return librosa.feature.chroma_cqt(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_cens(audio, sample_rate):\n",
    "    return librosa.feature.chroma_cens(y=audio, sr=sample_rate)\n",
    "\n",
    "def apply_specmix(mel_spec, label, train_features, train_labels, num_mixes=2, alpha=0.2):\n",
    "    # Get the indices of samples with the same label\n",
    "    same_label_indices = np.where(train_labels == label)[0]\n",
    "\n",
    "    if len(same_label_indices) < num_mixes:\n",
    "        # If there are not enough samples with the same label, use all available samples\n",
    "        mix_indices = same_label_indices\n",
    "    else:\n",
    "        # Randomly select num_mixes samples with the same label\n",
    "        mix_indices = np.random.choice(same_label_indices, size=num_mixes, replace=False)\n",
    "\n",
    "    # Get the mel spectrograms of the selected samples\n",
    "    mix_mel_specs = train_features[mix_indices]\n",
    "\n",
    "    # Generate mixing weights using the Beta distribution\n",
    "    weights = np.random.beta(alpha, alpha, size=len(mix_indices))\n",
    "    weights_norm = weights / np.sum(weights)\n",
    "\n",
    "    # Truncate or pad the selected mel spectrograms to match the shape of the input mel spectrogram\n",
    "    target_length = mel_spec.shape[1]\n",
    "    mix_mel_specs_resized = []\n",
    "    for spec in mix_mel_specs:\n",
    "        if len(spec.shape) == 1:\n",
    "            # If spec is 1-dimensional, reshape it to 2-dimensional\n",
    "            spec = spec.reshape(1, -1)\n",
    "        if spec.shape[1] > target_length:\n",
    "            # Truncate the spectrogram if it is longer than the target length\n",
    "            spec = spec[:, :target_length]\n",
    "        elif spec.shape[1] < target_length:\n",
    "            # Pad the spectrogram with zeros if it is shorter than the target length\n",
    "            pad_width = target_length - spec.shape[1]\n",
    "            spec = np.pad(spec, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        mix_mel_specs_resized.append(spec)\n",
    "\n",
    "    # Mix the mel spectrograms using the generated weights\n",
    "    mixed_mel_spec = np.zeros_like(mel_spec)\n",
    "    for i in range(len(mix_indices)):\n",
    "        mixed_mel_spec += weights_norm[i] * mix_mel_specs_resized[i]\n",
    "\n",
    "    return mixed_mel_spec\n",
    "\n",
    "def apply_audio_augmentation(mel_spec, label, train_features, train_labels):\n",
    "    augmented_mel_spec = apply_specmix(mel_spec, label, train_features, train_labels)\n",
    "    return augmented_mel_spec\n",
    "\n",
    "def preprocess_audio(file_path, label, train_features, train_labels, augment=False):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        print(f\"Loaded audio file: {file_path}\")\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128)\n",
    "\n",
    "        if augment and label is not None and train_features is not None and train_labels is not None:\n",
    "            augmented_mel_spec = apply_audio_augmentation(mel_spec, label, train_features, train_labels)\n",
    "            features_scaled = np.mean(augmented_mel_spec.T, axis=0)\n",
    "        else:\n",
    "            features_scaled = np.mean(mel_spec.T, axis=0)\n",
    "\n",
    "        print(f\"Extracted features: {features_scaled.shape}\")\n",
    "        return features_scaled\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_file(file_path, label, train_features, train_labels, augment=False):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    if label is None:\n",
    "        # If label is None, pass None for train_features and train_labels as well\n",
    "        features = preprocess_audio(file_path, None, None, None, augment=augment)\n",
    "    else:\n",
    "        features = preprocess_audio(file_path, label, train_features, train_labels, augment=augment)\n",
    "    return features\n",
    "\n",
    "def prepare_data(directory, train_features, train_labels, augment=False):\n",
    "    file_paths = [os.path.join(directory, f\"{i}.mp3\") for i in range(len(os.listdir(directory)))]\n",
    "    labels = train_labels.tolist() if train_labels is not None else [None] * len(file_paths)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        results = list(executor.map(lambda x: process_file(x[0], x[1], train_features, train_labels, augment), zip(file_paths, labels)))\n",
    "    \n",
    "    features = [feat for feat in results if feat is not None]\n",
    "    \n",
    "    if not features:\n",
    "        raise ValueError(\"No audio files were successfully processed.\")\n",
    "    \n",
    "    features = np.array(features)\n",
    "    print(f\"Processed {len(features)} audio files\")\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data from the 'V6' folder.\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Preparing Data (modified)\n",
    "folder_path = 'V6'\n",
    "\n",
    "try:\n",
    "    train_features, train_labels, test_features = load_preprocessed_data(folder_path)\n",
    "    print(\"Loaded preprocessed data from the 'V6' folder.\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "    train_features_original = prepare_data('train_mp3s/train_mp3s', np.empty((0, 128)), train_labels_encoded)\n",
    "    print(f\"Original train features shape: {train_features_original.shape}\")\n",
    "\n",
    "    train_features_augmented = prepare_data('train_mp3s/train_mp3s', train_features_original, train_labels_encoded, augment=True)\n",
    "    print(f\"Augmented train features shape: {train_features_augmented.shape}\")\n",
    "\n",
    "    train_features = np.concatenate((train_features_original, train_features_augmented), axis=0)\n",
    "    print(f\"Combined train features shape: {train_features.shape}\")\n",
    "\n",
    "    test_features = prepare_data('test_mp3s/test_mp3s', np.empty((0, 128)), None)\n",
    "    print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "    train_labels_augmented = train_labels_encoded.copy()\n",
    "    train_labels_encoded = np.concatenate((train_labels_encoded, train_labels_augmented), axis=0)\n",
    "    print(f\"Train labels shape: {train_labels_encoded.shape}\")\n",
    "\n",
    "    print(f\"Number of training features: {len(train_features)}\")\n",
    "    print(f\"Number of training labels: {len(train_labels_encoded)}\")\n",
    "    print(f\"Number of test features: {len(test_features)}\")\n",
    "\n",
    "    save_preprocessed_data(train_features, train_labels_encoded, test_features, folder_path)\n",
    "    print(f\"Saved preprocessed data to the {folder_path} folder.\")\n",
    "\n",
    "if len(train_features) == 0:\n",
    "    print(\"No training features available. Please check the data.\")\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_features_val, val_features, train_labels_encoded_val, val_labels_encoded = train_test_split(\n",
    "    train_features, train_labels_encoded, test_size=0.2, stratify=train_labels_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# Reshape the features to have a channel dimension of 1\n",
    "train_features_val = train_features_val.reshape(-1, 1, train_features_val.shape[1])\n",
    "val_features = val_features.reshape(-1, 1, val_features.shape[1])\n",
    "test_features = test_features.reshape(-1, 1, test_features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 513207.7228, Accuracy: 0.3018, F1 Score: 0.1935, Precision: 0.6206, Recall: 0.3018\n",
      "Epoch [2/500], Loss: 956.0677, Accuracy: 0.3437, F1 Score: 0.2290, Precision: 0.4806, Recall: 0.3437\n",
      "Epoch [3/500], Loss: 1209.0304, Accuracy: 0.3577, F1 Score: 0.2791, Precision: 0.3031, Recall: 0.3577\n",
      "Epoch [4/500], Loss: 4.8344, Accuracy: 0.2649, F1 Score: 0.1569, Precision: 0.3231, Recall: 0.2649\n",
      "Epoch [5/500], Loss: 1.6084, Accuracy: 0.2670, F1 Score: 0.1294, Precision: 0.3838, Recall: 0.2670\n",
      "Epoch [6/500], Loss: 1.4606, Accuracy: 0.2611, F1 Score: 0.1115, Precision: 0.3809, Recall: 0.2611\n",
      "Epoch [7/500], Loss: 1.3864, Accuracy: 0.2761, F1 Score: 0.1201, Precision: 0.3831, Recall: 0.2761\n",
      "Epoch [8/500], Loss: 1.3457, Accuracy: 0.2871, F1 Score: 0.1281, Precision: 0.7953, Recall: 0.2871\n",
      "Epoch [9/500], Loss: 1.3202, Accuracy: 0.2861, F1 Score: 0.1273, Precision: 0.7958, Recall: 0.2861\n",
      "Epoch [10/500], Loss: 1.3006, Accuracy: 0.2856, F1 Score: 0.1269, Precision: 0.7960, Recall: 0.2856\n",
      "Epoch [11/500], Loss: 1.2966, Accuracy: 0.3637, F1 Score: 0.1940, Precision: 0.7686, Recall: 0.3637\n",
      "Epoch [12/500], Loss: 1.3005, Accuracy: 0.3608, F1 Score: 0.1914, Precision: 0.4795, Recall: 0.3608\n",
      "Epoch [13/500], Loss: 1.2989, Accuracy: 0.3669, F1 Score: 0.1984, Precision: 0.6175, Recall: 0.3669\n",
      "Epoch [14/500], Loss: 1.2990, Accuracy: 0.3654, F1 Score: 0.1956, Precision: 0.7681, Recall: 0.3654\n",
      "Epoch [15/500], Loss: 1.2948, Accuracy: 0.3644, F1 Score: 0.1947, Precision: 0.7684, Recall: 0.3644\n",
      "Epoch [16/500], Loss: 1.2935, Accuracy: 0.3652, F1 Score: 0.1954, Precision: 0.7682, Recall: 0.3652\n",
      "Epoch [17/500], Loss: 1.2911, Accuracy: 0.3654, F1 Score: 0.1956, Precision: 0.7681, Recall: 0.3654\n",
      "Epoch [18/500], Loss: 1.2896, Accuracy: 0.3666, F1 Score: 0.1967, Precision: 0.7678, Recall: 0.3666\n",
      "Epoch [19/500], Loss: 1.2916, Accuracy: 0.3642, F1 Score: 0.1945, Precision: 0.7684, Recall: 0.3642\n",
      "Epoch [20/500], Loss: 1.2909, Accuracy: 0.3648, F1 Score: 0.1951, Precision: 0.7683, Recall: 0.3648\n",
      "Epoch [21/500], Loss: 1.2903, Accuracy: 0.3655, F1 Score: 0.1957, Precision: 0.7681, Recall: 0.3655\n",
      "Epoch [22/500], Loss: 1.2906, Accuracy: 0.3638, F1 Score: 0.1941, Precision: 0.7686, Recall: 0.3638\n",
      "Epoch [23/500], Loss: 1.2886, Accuracy: 0.3648, F1 Score: 0.1950, Precision: 0.7683, Recall: 0.3648\n",
      "Epoch [24/500], Loss: 1.2900, Accuracy: 0.3631, F1 Score: 0.1934, Precision: 0.7688, Recall: 0.3631\n",
      "Epoch [25/500], Loss: 1.2904, Accuracy: 0.3615, F1 Score: 0.1920, Precision: 0.7692, Recall: 0.3615\n",
      "Epoch [26/500], Loss: 1.2874, Accuracy: 0.3665, F1 Score: 0.1966, Precision: 0.7679, Recall: 0.3665\n",
      "Epoch [27/500], Loss: 1.2892, Accuracy: 0.3647, F1 Score: 0.1949, Precision: 0.7683, Recall: 0.3647\n",
      "Epoch [28/500], Loss: 1.2893, Accuracy: 0.3647, F1 Score: 0.1949, Precision: 0.7683, Recall: 0.3647\n",
      "Epoch [29/500], Loss: 1.2883, Accuracy: 0.3654, F1 Score: 0.1956, Precision: 0.7681, Recall: 0.3654\n",
      "Epoch [30/500], Loss: 1.2905, Accuracy: 0.3635, F1 Score: 0.1938, Precision: 0.7686, Recall: 0.3635\n",
      "Epoch [31/500], Loss: 1.2885, Accuracy: 0.3678, F1 Score: 0.1978, Precision: 0.7676, Recall: 0.3678\n",
      "Epoch [32/500], Loss: 1.2880, Accuracy: 0.3645, F1 Score: 0.1948, Precision: 0.7684, Recall: 0.3645\n",
      "Epoch [33/500], Loss: 1.2905, Accuracy: 0.3641, F1 Score: 0.1943, Precision: 0.7685, Recall: 0.3641\n",
      "Epoch [34/500], Loss: 1.2892, Accuracy: 0.3661, F1 Score: 0.1963, Precision: 0.7679, Recall: 0.3661\n",
      "Epoch [35/500], Loss: 1.2915, Accuracy: 0.3620, F1 Score: 0.1925, Precision: 0.7691, Recall: 0.3620\n",
      "Epoch [36/500], Loss: 1.2879, Accuracy: 0.3648, F1 Score: 0.1950, Precision: 0.7683, Recall: 0.3648\n",
      "Epoch [37/500], Loss: 1.2885, Accuracy: 0.3631, F1 Score: 0.1934, Precision: 0.7688, Recall: 0.3631\n",
      "Epoch [38/500], Loss: 1.2881, Accuracy: 0.3647, F1 Score: 0.1949, Precision: 0.7683, Recall: 0.3647\n",
      "Epoch [39/500], Loss: 1.2897, Accuracy: 0.3663, F1 Score: 0.1964, Precision: 0.7679, Recall: 0.3663\n",
      "Epoch [40/500], Loss: 1.2915, Accuracy: 0.3638, F1 Score: 0.1941, Precision: 0.7685, Recall: 0.3638\n",
      "Epoch [41/500], Loss: 1.2874, Accuracy: 0.3655, F1 Score: 0.1957, Precision: 0.7681, Recall: 0.3655\n",
      "Epoch [42/500], Loss: 1.2880, Accuracy: 0.3656, F1 Score: 0.1958, Precision: 0.7681, Recall: 0.3656\n",
      "Epoch [43/500], Loss: 1.2893, Accuracy: 0.3645, F1 Score: 0.1948, Precision: 0.7684, Recall: 0.3645\n",
      "Epoch [44/500], Loss: 1.2888, Accuracy: 0.3671, F1 Score: 0.1972, Precision: 0.7677, Recall: 0.3671\n",
      "Epoch [45/500], Loss: 1.2926, Accuracy: 0.3585, F1 Score: 0.1894, Precision: 0.7702, Recall: 0.3585\n",
      "Epoch [46/500], Loss: 1.2892, Accuracy: 0.3652, F1 Score: 0.1954, Precision: 0.7682, Recall: 0.3652\n",
      "Epoch [47/500], Loss: 1.2909, Accuracy: 0.3649, F1 Score: 0.1951, Precision: 0.7683, Recall: 0.3649\n",
      "Epoch [48/500], Loss: 1.2879, Accuracy: 0.3655, F1 Score: 0.1957, Precision: 0.7681, Recall: 0.3655\n",
      "Epoch [49/500], Loss: 1.2900, Accuracy: 0.3638, F1 Score: 0.1941, Precision: 0.7686, Recall: 0.3638\n",
      "Epoch [50/500], Loss: 1.2907, Accuracy: 0.3664, F1 Score: 0.1966, Precision: 0.7679, Recall: 0.3664\n",
      "Epoch [51/500], Loss: 1.2897, Accuracy: 0.3637, F1 Score: 0.1940, Precision: 0.7686, Recall: 0.3637\n",
      "Epoch [52/500], Loss: 1.2868, Accuracy: 0.3654, F1 Score: 0.1956, Precision: 0.7681, Recall: 0.3654\n",
      "Epoch [53/500], Loss: 1.2890, Accuracy: 0.3651, F1 Score: 0.1953, Precision: 0.7682, Recall: 0.3651\n",
      "Epoch [54/500], Loss: 1.2889, Accuracy: 0.3684, F1 Score: 0.1985, Precision: 0.7674, Recall: 0.3684\n",
      "Epoch [55/500], Loss: 1.2890, Accuracy: 0.3615, F1 Score: 0.1920, Precision: 0.7692, Recall: 0.3615\n",
      "Epoch [56/500], Loss: 1.2892, Accuracy: 0.3666, F1 Score: 0.1967, Precision: 0.7678, Recall: 0.3666\n",
      "Epoch [57/500], Loss: 1.2927, Accuracy: 0.3632, F1 Score: 0.1936, Precision: 0.7688, Recall: 0.3632\n",
      "Epoch [58/500], Loss: 1.2918, Accuracy: 0.3629, F1 Score: 0.1933, Precision: 0.7688, Recall: 0.3629\n",
      "Epoch [59/500], Loss: 1.2892, Accuracy: 0.3661, F1 Score: 0.1963, Precision: 0.7679, Recall: 0.3661\n",
      "Epoch [60/500], Loss: 1.2902, Accuracy: 0.3627, F1 Score: 0.1931, Precision: 0.7689, Recall: 0.3627\n",
      "Epoch [61/500], Loss: 1.2894, Accuracy: 0.3635, F1 Score: 0.1938, Precision: 0.7686, Recall: 0.3635\n",
      "Epoch [62/500], Loss: 1.2911, Accuracy: 0.3629, F1 Score: 0.1933, Precision: 0.7688, Recall: 0.3629\n",
      "Epoch [63/500], Loss: 1.2882, Accuracy: 0.3610, F1 Score: 0.1916, Precision: 0.7694, Recall: 0.3610\n",
      "Epoch [64/500], Loss: 1.2888, Accuracy: 0.3661, F1 Score: 0.1962, Precision: 0.7680, Recall: 0.3661\n",
      "Epoch [65/500], Loss: 1.2913, Accuracy: 0.3649, F1 Score: 0.1951, Precision: 0.7683, Recall: 0.3649\n",
      "Epoch [66/500], Loss: 1.2886, Accuracy: 0.3648, F1 Score: 0.1950, Precision: 0.7683, Recall: 0.3648\n",
      "Early stopping at epoch 66\n"
     ]
    }
   ],
   "source": [
    "# Block 5: Model Definition, Training, and Prediction\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * input_size, hidden_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, device, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        running_f1 = 0.0\n",
    "        running_precision = 0.0\n",
    "        running_recall = 0.0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            running_accuracy += accuracy_score(labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "            running_f1 += f1_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='weighted', zero_division=1)\n",
    "            running_precision += precision_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='weighted', zero_division=1)\n",
    "            running_recall += recall_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='weighted', zero_division=1)\n",
    "        \n",
    "        scheduler.step()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = running_accuracy / len(train_loader)\n",
    "        epoch_f1 = running_f1 / len(train_loader)\n",
    "        epoch_precision = running_precision / len(train_loader)\n",
    "        epoch_recall = running_recall / len(train_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, F1 Score: {epoch_f1:.4f}, Precision: {epoch_precision:.4f}, Recall: {epoch_recall:.4f}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AudioDataset(train_features_val, train_labels_encoded_val)\n",
    "val_dataset = AudioDataset(val_features, val_labels_encoded)\n",
    "test_dataset = AudioDataset(test_features, np.zeros(len(test_features)))  # Dummy labels for test data\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8096\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Model parameters\n",
    "input_size = train_features_val.shape[2]  # Assuming the input features have shape (batch_size, 1, feature_size)\n",
    "hidden_size = 1024\n",
    "num_classes = len(np.unique(train_labels_encoded_val))\n",
    "num_epochs = 500\n",
    "\n",
    "# Create the model\n",
    "model = AudioClassifier(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001 * (batch_size / 64)  # Scale the learning rate based on the batch size\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "# Implement early stopping\n",
    "patience = 20\n",
    "best_validation_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    running_f1 = 0.0\n",
    "    running_precision = 0.0\n",
    "    running_recall = 0.0\n",
    "    \n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        running_accuracy += accuracy_score(labels.cpu().numpy(), predicted.cpu().numpy())\n",
    "        running_f1 += f1_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='weighted', zero_division=1)\n",
    "        running_precision += precision_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='weighted', zero_division=1)\n",
    "        running_recall += recall_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='weighted', zero_division=1)\n",
    "    \n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = running_accuracy / len(train_loader)\n",
    "    epoch_f1 = running_f1 / len(train_loader)\n",
    "    epoch_precision = running_precision / len(train_loader)\n",
    "    epoch_recall = running_recall / len(train_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, F1 Score: {epoch_f1:.4f}, Precision: {epoch_precision:.4f}, Recall: {epoch_recall:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            validation_loss += loss.item()\n",
    "    validation_loss /= len(val_loader)\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Generate submission.csv\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for features, _ in test_loader:\n",
    "        features = features.to(device)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "submission = pd.DataFrame({'id': range(len(predictions)), 'category': predictions})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
