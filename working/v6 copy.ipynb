{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4080 Laptop GPU is available.\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Importing Libraries\n",
    "import random\n",
    "import tarfile\n",
    "import resampy\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import albumentations as A\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping extraction of train_mp3s.tar.\n",
      "Skipping extraction of test_mp3s.tar.\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Extracting Data\n",
    "def extract_tar(tar_file, target_dir):\n",
    "    if os.path.exists(target_dir):\n",
    "        user_input = input(f\"The directory '{target_dir}' already exists. Do you want to skip extraction? (y/n): \")\n",
    "        if user_input.lower() == 'y':\n",
    "            print(f\"Skipping extraction of {tar_file}.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Overwriting the existing directory '{target_dir}'.\")\n",
    "            shutil.rmtree(target_dir)\n",
    "    with tarfile.open(tar_file, 'r') as tar:\n",
    "        tar.extractall(target_dir)\n",
    "    # Remove residue \"._\" hidden files from the inner folder\n",
    "    inner_folder = os.path.join(target_dir, os.path.splitext(os.path.basename(tar_file))[0])\n",
    "    for root, dirs, files in os.walk(inner_folder):\n",
    "        for file in files:\n",
    "            if file.startswith(\"._\"):\n",
    "                os.remove(os.path.join(root, file))\n",
    "\n",
    "extract_tar('train_mp3s.tar', 'train_mp3s')\n",
    "extract_tar('test_mp3s.tar', 'test_mp3s')\n",
    "train_labels = np.loadtxt('train_label.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Preprocessing Functions\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_preprocessed_data(train_features, train_labels, test_features, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_features.pkl'), 'wb') as f:\n",
    "        pickle.dump(train_features, f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_labels.pkl'), 'wb') as f:\n",
    "        pickle.dump(train_labels, f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'test_features.pkl'), 'wb') as f:\n",
    "        pickle.dump(test_features, f)\n",
    "\n",
    "def load_preprocessed_data(folder_path):\n",
    "    with open(os.path.join(folder_path, 'train_features.pkl'), 'rb') as f:\n",
    "        train_features = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'train_labels.pkl'), 'rb') as f:\n",
    "        train_labels = pickle.load(f)\n",
    "\n",
    "    with open(os.path.join(folder_path, 'test_features.pkl'), 'rb') as f:\n",
    "        test_features = pickle.load(f)\n",
    "\n",
    "    return train_features, train_labels, test_features\n",
    "def extract_mfcc(audio, sample_rate):\n",
    "    return librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "\n",
    "def extract_mel_spec(audio, sample_rate):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_scaled = np.mean(mel_spec_db.T, axis=0)\n",
    "    return mel_spec_scaled\n",
    "\n",
    "def extract_tonnetz(audio, sample_rate):\n",
    "    return librosa.feature.tonnetz(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_stft(audio, sample_rate):\n",
    "    return librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_cqt(audio, sample_rate):\n",
    "    return librosa.feature.chroma_cqt(y=audio, sr=sample_rate)\n",
    "\n",
    "def extract_chroma_cens(audio, sample_rate):\n",
    "    return librosa.feature.chroma_cens(y=audio, sr=sample_rate)\n",
    "def apply_audio_augmentation(mel_spectrogram):\n",
    "    # Get the shape of the mel spectrogram\n",
    "    F, T = mel_spectrogram.shape\n",
    "\n",
    "    # Create a binary mask filled with ones\n",
    "    M = np.ones((F, T), dtype=int)\n",
    "\n",
    "    # Frequency masking\n",
    "    ftimes = random.randint(0, 3)\n",
    "    for _ in range(ftimes):\n",
    "        fstart = random.randint(0, F - 1)\n",
    "        γ = random.uniform(0, 1)\n",
    "        fend = int(fstart + γ * F)\n",
    "        M[fstart:fend, :] = 0\n",
    "\n",
    "    # Time masking\n",
    "    ttimes = random.randint(0, 3)\n",
    "    for _ in range(ttimes):\n",
    "        tstart = random.randint(0, T - 1)\n",
    "        γ = random.uniform(0, 1)\n",
    "        tend = int(tstart + γ * T)\n",
    "        M[:, tstart:tend] = 0\n",
    "\n",
    "    # Apply the mask to the mel spectrogram\n",
    "    masked_mel_spectrogram = M * mel_spectrogram\n",
    "\n",
    "    return masked_mel_spectrogram\n",
    "def preprocess_audio(file_path, augment=False):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        print(f\"Loaded audio file: {file_path}\")\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        if augment:\n",
    "            augmented_mel_spec_db = apply_audio_augmentation(mel_spec_db)\n",
    "            features_scaled = np.mean(augmented_mel_spec_db.T, axis=0)\n",
    "        else:\n",
    "            features_scaled = np.mean(mel_spec_db.T, axis=0)\n",
    "\n",
    "        print(f\"Extracted features: {features_scaled.shape}\")\n",
    "        return features_scaled\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def process_file(file_path, augment=False):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    features = preprocess_audio(file_path, augment=augment)\n",
    "    return features\n",
    "\n",
    "def prepare_data(directory, augment=False):\n",
    "    file_paths = [os.path.join(directory, f\"{i}.mp3\") for i in range(len(os.listdir(directory)))]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "        results = list(executor.map(lambda x: process_file(x, augment), file_paths))\n",
    "    \n",
    "    features = []\n",
    "    for feat in results:\n",
    "        if feat is not None:\n",
    "            features.append(feat)\n",
    "        else:\n",
    "            features.append(np.zeros_like(features[0]))  # Append a placeholder if augmented features are not generated\n",
    "    \n",
    "    features = np.array(features)\n",
    "    print(f\"Processed {len(features)} audio files\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data from the 'V6' folder.\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Preparing Data (modified)\n",
    "\n",
    "folder_path = 'V6'\n",
    "\n",
    "try:\n",
    "    train_features, train_labels, test_features = load_preprocessed_data(folder_path)\n",
    "    print(\"Loaded preprocessed data from the 'V6' folder.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    train_features_original = prepare_data('train_mp3s/train_mp3s')\n",
    "    print(f\"Original train features shape: {train_features_original.shape}\")\n",
    "\n",
    "    train_features_augmented = prepare_data('train_mp3s/train_mp3s', augment=True)\n",
    "    print(f\"Augmented train features shape: {train_features_augmented.shape}\")\n",
    "\n",
    "    train_features = np.concatenate((train_features_original, train_features_augmented), axis=0)\n",
    "    print(f\"Combined train features shape: {train_features.shape}\")\n",
    "\n",
    "    test_features = prepare_data('test_mp3s/test_mp3s')\n",
    "    print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels_original = label_encoder.fit_transform(train_labels)\n",
    "    train_labels_augmented = train_labels_original.copy()\n",
    "    train_labels = np.concatenate((train_labels_original, train_labels_augmented), axis=0)\n",
    "    print(f\"Train labels shape: {train_labels.shape}\")\n",
    "\n",
    "    print(f\"Number of training features: {len(train_features)}\")\n",
    "    print(f\"Number of training labels: {len(train_labels)}\")\n",
    "    print(f\"Number of test features: {len(test_features)}\")\n",
    "\n",
    "    save_preprocessed_data(train_features, train_labels, test_features, folder_path)\n",
    "    print(f\"Saved preprocessed data to the {folder_path} folder.\")\n",
    "\n",
    "if len(train_features) == 0:\n",
    "    print(\"No training features available. Please check the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.1768\n",
      "Epoch [2/50], Loss: 1.0859\n",
      "Epoch [3/50], Loss: 1.0610\n",
      "Epoch [4/50], Loss: 1.0418\n",
      "Epoch [5/50], Loss: 1.0277\n",
      "Epoch [6/50], Loss: 1.0109\n",
      "Epoch [7/50], Loss: 1.0048\n",
      "Epoch [8/50], Loss: 0.9911\n",
      "Epoch [9/50], Loss: 0.9934\n",
      "Epoch [10/50], Loss: 0.9860\n",
      "Epoch [11/50], Loss: 0.9770\n",
      "Epoch [12/50], Loss: 0.9663\n",
      "Epoch [13/50], Loss: 0.9606\n",
      "Epoch [14/50], Loss: 0.9524\n",
      "Epoch [15/50], Loss: 0.9543\n",
      "Epoch [16/50], Loss: 0.9469\n",
      "Epoch [17/50], Loss: 0.9428\n",
      "Epoch [18/50], Loss: 0.9368\n",
      "Epoch [19/50], Loss: 0.9254\n",
      "Epoch [20/50], Loss: 0.9250\n",
      "Epoch [21/50], Loss: 0.9148\n",
      "Epoch [22/50], Loss: 0.9164\n",
      "Epoch [23/50], Loss: 0.9120\n",
      "Epoch [24/50], Loss: 0.9071\n",
      "Epoch [25/50], Loss: 0.9060\n",
      "Epoch [26/50], Loss: 0.8949\n",
      "Epoch [27/50], Loss: 0.8935\n",
      "Epoch [28/50], Loss: 0.8871\n",
      "Epoch [29/50], Loss: 0.8846\n",
      "Epoch [30/50], Loss: 0.8858\n",
      "Epoch [31/50], Loss: 0.8831\n",
      "Epoch [32/50], Loss: 0.8781\n",
      "Epoch [33/50], Loss: 0.8699\n",
      "Epoch [34/50], Loss: 0.8706\n",
      "Epoch [35/50], Loss: 0.8702\n",
      "Epoch [36/50], Loss: 0.8632\n",
      "Epoch [37/50], Loss: 0.8578\n",
      "Epoch [38/50], Loss: 0.8554\n",
      "Epoch [39/50], Loss: 0.8555\n",
      "Epoch [40/50], Loss: 0.8522\n",
      "Epoch [41/50], Loss: 0.8501\n",
      "Epoch [42/50], Loss: 0.8504\n",
      "Epoch [43/50], Loss: 0.8490\n",
      "Epoch [44/50], Loss: 0.8462\n",
      "Epoch [45/50], Loss: 0.8425\n",
      "Epoch [46/50], Loss: 0.8451\n",
      "Epoch [47/50], Loss: 0.8402\n",
      "Epoch [48/50], Loss: 0.8405\n",
      "Epoch [49/50], Loss: 0.8465\n",
      "Epoch [50/50], Loss: 0.8389\n"
     ]
    }
   ],
   "source": [
    "# Block 5: Model Definition, Training, and Prediction\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = features.astype(np.float32)  # Convert features to float32\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        else:\n",
    "            return self.features[idx]\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / (batch_idx+1):.4f}\")\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "# Set hyperparameters\n",
    "input_size = train_features.shape[1]\n",
    "hidden_size = 256\n",
    "num_classes = len(np.unique(train_labels))\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 150\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = AudioDataset(train_features, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = AudioDataset(test_features)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model, loss function, optimizer, and scheduler\n",
    "model = AudioClassifier(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = predict(model, test_loader)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "output_df = pd.DataFrame({'id': range(len(predictions)), 'category': predictions})\n",
    "output_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
