{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4080 Laptop GPU is available.\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Importing Libraries\n",
    "import tarfile\n",
    "import resampy\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping extraction of train_mp3s.tar.\n",
      "Skipping extraction of test_mp3s.tar.\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Extracting Data\n",
    "def extract_tar(tar_file, target_dir):\n",
    "    if os.path.exists(target_dir):\n",
    "        user_input = input(f\"The directory '{target_dir}' already exists. Do you want to skip extraction? (y/n): \")\n",
    "        if user_input.lower() == 'y':\n",
    "            print(f\"Skipping extraction of {tar_file}.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Overwriting the existing directory '{target_dir}'.\")\n",
    "            shutil.rmtree(target_dir)\n",
    "\n",
    "    with tarfile.open(tar_file, 'r') as tar:\n",
    "        tar.extractall(target_dir)\n",
    "\n",
    "    # Remove residue \"._\" hidden files from the inner folder\n",
    "    inner_folder = os.path.join(target_dir, os.path.splitext(os.path.basename(tar_file))[0])\n",
    "    for root, dirs, files in os.walk(inner_folder):\n",
    "        for file in files:\n",
    "            if file.startswith(\"._\"):\n",
    "                os.remove(os.path.join(root, file))\n",
    "\n",
    "extract_tar('train_mp3s.tar', 'train_mp3s')\n",
    "extract_tar('test_mp3s.tar', 'test_mp3s')\n",
    "train_labels = np.loadtxt('train_label.txt', dtype=int)\n",
    "train_labels = np.array([int(label) for label in train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Preprocessing Functions\n",
    "def preprocess_audio(file_path):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        print(f\"Loaded audio file: {file_path}\")\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
    "        print(f\"Extracted MFCCs: {mfccs_scaled.shape}\")\n",
    "        return mfccs_scaled\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_file(file_path):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    mfccs = preprocess_audio(file_path)\n",
    "    return mfccs\n",
    "\n",
    "def prepare_data(directory):\n",
    "    file_paths = [os.path.join(directory, f\"{i}.mp3\") for i in range(len(os.listdir(directory)))]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_file, file_paths))\n",
    "    features = [mfccs for mfccs in results if mfccs is not None]\n",
    "    print(f\"Processed {len(features)} audio files\")\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train features from file.\n",
      "Loaded test features from file.\n",
      "Train features shape: (11886, 40)\n",
      "Test features shape: (2447, 40)\n",
      "Train labels shape: (11886,)\n",
      "Number of training features: 11886\n",
      "Number of training labels: 11886\n",
      "Number of test features: 2447\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Preparing Data\n",
    "train_features_file = 'train_features.pkl'\n",
    "test_features_file = 'test_features.pkl'\n",
    "\n",
    "try:\n",
    "    with open(train_features_file, 'rb') as f:\n",
    "        train_features = pickle.load(f)\n",
    "    print(\"Loaded train features from file.\")\n",
    "except FileNotFoundError:\n",
    "    train_features = prepare_data('train_mp3s/train_mp3s')\n",
    "    with open(train_features_file, 'wb') as f:\n",
    "        pickle.dump(train_features, f)\n",
    "    print(\"Saved train features to file.\")\n",
    "\n",
    "try:\n",
    "    with open(test_features_file, 'rb') as f:\n",
    "        test_features = pickle.load(f)\n",
    "    print(\"Loaded test features from file.\")\n",
    "except FileNotFoundError:\n",
    "    test_features = prepare_data('test_mp3s/test_mp3s')\n",
    "    with open(test_features_file, 'wb') as f:\n",
    "        pickle.dump(test_features, f)\n",
    "    print(\"Saved test features to file.\")\n",
    "\n",
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "train_labels = np.array([int(label) for label in train_labels])\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Number of training features: {len(train_features)}\")\n",
    "print(f\"Number of training labels: {len(train_labels)}\")\n",
    "print(f\"Number of test features: {len(test_features)}\")\n",
    "\n",
    "if len(train_features) == 0:\n",
    "    print(\"No training features available. Please check the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mojii\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Validation Loss: 1.4152, Validation Accuracy: 0.0945\n",
      "Epoch [2/10000], Validation Loss: 1.2493, Validation Accuracy: 0.4675\n",
      "Epoch [3/10000], Validation Loss: 1.0845, Validation Accuracy: 0.5308\n",
      "Epoch [4/10000], Validation Loss: 0.9954, Validation Accuracy: 0.5757\n",
      "Epoch [5/10000], Validation Loss: 0.9127, Validation Accuracy: 0.6268\n",
      "Epoch [6/10000], Validation Loss: 0.8411, Validation Accuracy: 0.6469\n",
      "Epoch [7/10000], Validation Loss: 0.7946, Validation Accuracy: 0.6626\n",
      "Epoch [8/10000], Validation Loss: 0.7593, Validation Accuracy: 0.6803\n",
      "Epoch [9/10000], Validation Loss: 0.7387, Validation Accuracy: 0.6884\n",
      "Epoch [10/10000], Validation Loss: 0.7047, Validation Accuracy: 0.7013\n",
      "Epoch [11/10000], Validation Loss: 0.6923, Validation Accuracy: 0.7061\n",
      "Epoch [12/10000], Validation Loss: 0.6843, Validation Accuracy: 0.7114\n",
      "Epoch [13/10000], Validation Loss: 0.6829, Validation Accuracy: 0.7142\n",
      "Epoch [14/10000], Validation Loss: 0.6675, Validation Accuracy: 0.7238\n",
      "Epoch [15/10000], Validation Loss: 0.6584, Validation Accuracy: 0.7302\n",
      "Epoch [16/10000], Validation Loss: 0.6422, Validation Accuracy: 0.7426\n",
      "Epoch [17/10000], Validation Loss: 0.6419, Validation Accuracy: 0.7347\n",
      "Epoch [18/10000], Validation Loss: 0.6395, Validation Accuracy: 0.7358\n",
      "Epoch [19/10000], Validation Loss: 0.6310, Validation Accuracy: 0.7420\n",
      "Epoch [20/10000], Validation Loss: 0.6275, Validation Accuracy: 0.7423\n",
      "Epoch [21/10000], Validation Loss: 0.6214, Validation Accuracy: 0.7443\n",
      "Epoch [22/10000], Validation Loss: 0.6264, Validation Accuracy: 0.7459\n",
      "Epoch [23/10000], Validation Loss: 0.6152, Validation Accuracy: 0.7504\n",
      "Epoch [24/10000], Validation Loss: 0.6130, Validation Accuracy: 0.7451\n",
      "Epoch [25/10000], Validation Loss: 0.6035, Validation Accuracy: 0.7499\n",
      "Epoch [26/10000], Validation Loss: 0.6040, Validation Accuracy: 0.7504\n",
      "Epoch [27/10000], Validation Loss: 0.6090, Validation Accuracy: 0.7473\n",
      "Epoch [28/10000], Validation Loss: 0.5973, Validation Accuracy: 0.7513\n",
      "Epoch [29/10000], Validation Loss: 0.5968, Validation Accuracy: 0.7504\n",
      "Epoch [30/10000], Validation Loss: 0.5968, Validation Accuracy: 0.7541\n",
      "Epoch [31/10000], Validation Loss: 0.5890, Validation Accuracy: 0.7557\n",
      "Epoch [32/10000], Validation Loss: 0.5875, Validation Accuracy: 0.7549\n",
      "Epoch [33/10000], Validation Loss: 0.5876, Validation Accuracy: 0.7563\n",
      "Epoch [34/10000], Validation Loss: 0.5855, Validation Accuracy: 0.7580\n",
      "Epoch [35/10000], Validation Loss: 0.5834, Validation Accuracy: 0.7555\n",
      "Epoch [36/10000], Validation Loss: 0.5754, Validation Accuracy: 0.7574\n",
      "Epoch [37/10000], Validation Loss: 0.5756, Validation Accuracy: 0.7636\n",
      "Epoch [38/10000], Validation Loss: 0.5715, Validation Accuracy: 0.7630\n",
      "Epoch [39/10000], Validation Loss: 0.5739, Validation Accuracy: 0.7625\n",
      "Epoch [40/10000], Validation Loss: 0.5690, Validation Accuracy: 0.7670\n",
      "Epoch [41/10000], Validation Loss: 0.5655, Validation Accuracy: 0.7709\n",
      "Epoch [42/10000], Validation Loss: 0.5664, Validation Accuracy: 0.7695\n",
      "Epoch [43/10000], Validation Loss: 0.5664, Validation Accuracy: 0.7715\n",
      "Epoch [44/10000], Validation Loss: 0.5605, Validation Accuracy: 0.7720\n",
      "Epoch [45/10000], Validation Loss: 0.5590, Validation Accuracy: 0.7717\n",
      "Epoch [46/10000], Validation Loss: 0.5609, Validation Accuracy: 0.7726\n",
      "Epoch [47/10000], Validation Loss: 0.5582, Validation Accuracy: 0.7695\n",
      "Epoch [48/10000], Validation Loss: 0.5598, Validation Accuracy: 0.7712\n",
      "Epoch [49/10000], Validation Loss: 0.5552, Validation Accuracy: 0.7737\n",
      "Epoch [50/10000], Validation Loss: 0.5487, Validation Accuracy: 0.7745\n",
      "Epoch [51/10000], Validation Loss: 0.5452, Validation Accuracy: 0.7726\n",
      "Epoch [52/10000], Validation Loss: 0.5488, Validation Accuracy: 0.7731\n",
      "Epoch [53/10000], Validation Loss: 0.5439, Validation Accuracy: 0.7754\n",
      "Epoch [54/10000], Validation Loss: 0.5485, Validation Accuracy: 0.7776\n",
      "Epoch [55/10000], Validation Loss: 0.5440, Validation Accuracy: 0.7757\n",
      "Epoch [56/10000], Validation Loss: 0.5382, Validation Accuracy: 0.7762\n",
      "Epoch [57/10000], Validation Loss: 0.5451, Validation Accuracy: 0.7754\n",
      "Epoch [58/10000], Validation Loss: 0.5400, Validation Accuracy: 0.7793\n",
      "Epoch [59/10000], Validation Loss: 0.5383, Validation Accuracy: 0.7830\n",
      "Epoch [60/10000], Validation Loss: 0.5341, Validation Accuracy: 0.7827\n",
      "Epoch [61/10000], Validation Loss: 0.5351, Validation Accuracy: 0.7807\n",
      "Epoch [62/10000], Validation Loss: 0.5303, Validation Accuracy: 0.7824\n",
      "Epoch [63/10000], Validation Loss: 0.5301, Validation Accuracy: 0.7804\n",
      "Epoch [64/10000], Validation Loss: 0.5317, Validation Accuracy: 0.7801\n",
      "Epoch [65/10000], Validation Loss: 0.5333, Validation Accuracy: 0.7821\n",
      "Epoch [66/10000], Validation Loss: 0.5310, Validation Accuracy: 0.7821\n",
      "Epoch [67/10000], Validation Loss: 0.5229, Validation Accuracy: 0.7832\n",
      "Epoch [68/10000], Validation Loss: 0.5234, Validation Accuracy: 0.7832\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     76\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m val_labels)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m---> 77\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep(loss)\n\u001b[0;32m     80\u001b[0m test_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(test_features, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Block 5: Model Training and Prediction\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if len(train_features) > 0:\n",
    "    if len(train_features) != len(train_labels):\n",
    "        raise ValueError(\"Number of train features and labels do not match.\")\n",
    "\n",
    "    # Convert the input features to a PyTorch tensor\n",
    "    train_features = torch.tensor(train_features, dtype=torch.float32)\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "    # Define the model architecture\n",
    "    class AudioClassifier(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super(AudioClassifier, self).__init__()\n",
    "            self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.relu1 = nn.ReLU()\n",
    "            self.maxpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "            self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.maxpool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc1 = nn.Linear(640, 128)\n",
    "            self.relu3 = nn.ReLU()\n",
    "            self.dropout1 = nn.Dropout(0.5)\n",
    "            self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.view(x.size(0), 1, -1)  # Reshape the input tensor\n",
    "            x = self.conv1(x)\n",
    "            x = self.relu1(x)\n",
    "            x = self.maxpool1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.maxpool2(x)\n",
    "            x = self.flatten(x)\n",
    "            x = self.fc1(x)\n",
    "            x = self.relu3(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n",
    "    # Create an instance of the model\n",
    "    num_classes = 4  # Assuming 4 classes in the audio classification task\n",
    "    model = AudioClassifier(num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "    num_epochs = 400\n",
    "    batch_size = 128\n",
    "\n",
    "    # Use k-fold cross-validation\n",
    "    num_folds = 5\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_features)):\n",
    "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        train_data = train_features[train_idx].to(device)\n",
    "        train_labels_fold = train_labels[train_idx].to(device)\n",
    "        val_data = train_features[val_idx].to(device)\n",
    "        val_labels = train_labels[val_idx].to(device)\n",
    "\n",
    "        train_dataset = torch.utils.data.TensorDataset(train_data, train_labels_fold)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        best_val_loss = np.inf\n",
    "        patience = 20\n",
    "        counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch_data, batch_labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(val_data)\n",
    "                loss = criterion(outputs, val_labels)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                accuracy = (predicted == val_labels).float().mean()\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {loss.item():.4f}, Validation Accuracy: {accuracy.item():.4f}\")\n",
    "\n",
    "                if loss.item() < best_val_loss:\n",
    "                    best_val_loss = loss.item()\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    counter += 1\n",
    "\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "                scheduler.step(loss)\n",
    "\n",
    "    test_features = torch.tensor(test_features, dtype=torch.float32, device=device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_features)\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        predicted_labels = predicted_labels.cpu().tolist()\n",
    "\n",
    "    submission = pd.DataFrame({'id': range(len(predicted_labels)), 'category': predicted_labels})\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "else:\n",
    "    print(\"No training features available. Please check the data.\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
