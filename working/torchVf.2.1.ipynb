{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4080 Laptop GPU is available.\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Importing Libraries\n",
    "import tarfile\n",
    "import resampy\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping extraction of train_mp3s.tar.\n",
      "Skipping extraction of test_mp3s.tar.\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Extracting Data\n",
    "def extract_tar(tar_file, target_dir):\n",
    "    if os.path.exists(target_dir):\n",
    "        user_input = input(f\"The directory '{target_dir}' already exists. Do you want to skip extraction? (y/n): \")\n",
    "        if user_input.lower() == 'y':\n",
    "            print(f\"Skipping extraction of {tar_file}.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Overwriting the existing directory '{target_dir}'.\")\n",
    "            shutil.rmtree(target_dir)\n",
    "\n",
    "    with tarfile.open(tar_file, 'r') as tar:\n",
    "        tar.extractall(target_dir)\n",
    "\n",
    "    # Remove residue \"._\" hidden files from the inner folder\n",
    "    inner_folder = os.path.join(target_dir, os.path.splitext(os.path.basename(tar_file))[0])\n",
    "    for root, dirs, files in os.walk(inner_folder):\n",
    "        for file in files:\n",
    "            if file.startswith(\"._\"):\n",
    "                os.remove(os.path.join(root, file))\n",
    "\n",
    "extract_tar('train_mp3s.tar', 'train_mp3s')\n",
    "extract_tar('test_mp3s.tar', 'test_mp3s')\n",
    "train_labels = np.loadtxt('train_label.txt', dtype=int)\n",
    "train_labels = np.array([int(label) for label in train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Preprocessing Functions\n",
    "def preprocess_audio(file_path):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        print(f\"Loaded audio file: {file_path}\")\n",
    "        mfccs = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128)\n",
    "        mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
    "        print(f\"Extracted MFCCs: {mfccs_scaled.shape}\")\n",
    "        return mfccs_scaled\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_file(file_path):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    mfccs = preprocess_audio(file_path)\n",
    "    return mfccs\n",
    "\n",
    "def prepare_data(directory):\n",
    "    file_paths = [os.path.join(directory, f\"{i}.mp3\") for i in range(len(os.listdir(directory)))]\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_file, file_paths))\n",
    "    features = [mfccs for mfccs in results if mfccs is not None]\n",
    "    print(f\"Processed {len(features)} audio files\")\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train features from file.\n",
      "Loaded test features from file.\n",
      "Train features shape: (11886, 40)\n",
      "Test features shape: (2447, 40)\n",
      "Train labels shape: (11886,)\n",
      "Number of training features: 11886\n",
      "Number of training labels: 11886\n",
      "Number of test features: 2447\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Preparing Data\n",
    "train_features_file = 'train_features.pkl'\n",
    "test_features_file = 'test_features.pkl'\n",
    "\n",
    "try:\n",
    "    with open(train_features_file, 'rb') as f:\n",
    "        train_features = pickle.load(f)\n",
    "    print(\"Loaded train features from file.\")\n",
    "except FileNotFoundError:\n",
    "    train_features = prepare_data('train_mp3s/train_mp3s')\n",
    "    with open(train_features_file, 'wb') as f:\n",
    "        pickle.dump(train_features, f)\n",
    "    print(\"Saved train features to file.\")\n",
    "\n",
    "try:\n",
    "    with open(test_features_file, 'rb') as f:\n",
    "        test_features = pickle.load(f)\n",
    "    print(\"Loaded test features from file.\")\n",
    "except FileNotFoundError:\n",
    "    test_features = prepare_data('test_mp3s/test_mp3s')\n",
    "    with open(test_features_file, 'wb') as f:\n",
    "        pickle.dump(test_features, f)\n",
    "    print(\"Saved test features to file.\")\n",
    "\n",
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "\n",
    "train_labels = np.array([int(label) for label in train_labels])\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Number of training features: {len(train_features)}\")\n",
    "print(f\"Number of training labels: {len(train_labels)}\")\n",
    "print(f\"Number of test features: {len(test_features)}\")\n",
    "\n",
    "if len(train_features) == 0:\n",
    "    print(\"No training features available. Please check the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/500] - Train Loss: 1.3022, Train Accuracy: 0.3377\n",
      "Epoch [2/500] - Train Loss: 1.2135, Train Accuracy: 0.4001\n",
      "Epoch [3/500] - Train Loss: 1.1308, Train Accuracy: 0.4521\n",
      "Epoch [4/500] - Train Loss: 1.0130, Train Accuracy: 0.5547\n",
      "Epoch [5/500] - Train Loss: 0.9565, Train Accuracy: 0.5862\n",
      "Epoch [6/500] - Train Loss: 0.9254, Train Accuracy: 0.6007\n",
      "Epoch [7/500] - Train Loss: 0.9151, Train Accuracy: 0.6104\n",
      "Epoch [8/500] - Train Loss: 0.8990, Train Accuracy: 0.6103\n",
      "Epoch [9/500] - Train Loss: 0.8677, Train Accuracy: 0.6257\n",
      "Epoch [10/500] - Train Loss: 0.8511, Train Accuracy: 0.6279\n",
      "Epoch [11/500] - Train Loss: 0.8478, Train Accuracy: 0.6259\n",
      "Epoch [12/500] - Train Loss: 0.8375, Train Accuracy: 0.6438\n",
      "Epoch [13/500] - Train Loss: 0.8220, Train Accuracy: 0.6394\n",
      "Epoch [14/500] - Train Loss: 0.8186, Train Accuracy: 0.6437\n",
      "Epoch [15/500] - Train Loss: 0.7919, Train Accuracy: 0.6566\n",
      "Epoch [16/500] - Train Loss: 0.7801, Train Accuracy: 0.6619\n",
      "Epoch [17/500] - Train Loss: 0.7764, Train Accuracy: 0.6628\n",
      "Epoch [18/500] - Train Loss: 0.7782, Train Accuracy: 0.6572\n",
      "Epoch [19/500] - Train Loss: 0.7395, Train Accuracy: 0.6732\n",
      "Epoch [20/500] - Train Loss: 0.7477, Train Accuracy: 0.6724\n",
      "Epoch [21/500] - Train Loss: 0.7461, Train Accuracy: 0.6735\n",
      "Epoch [22/500] - Train Loss: 0.7167, Train Accuracy: 0.6855\n",
      "Epoch [23/500] - Train Loss: 0.7341, Train Accuracy: 0.6727\n",
      "Epoch [24/500] - Train Loss: 0.7108, Train Accuracy: 0.6855\n",
      "Epoch [25/500] - Train Loss: 0.7020, Train Accuracy: 0.6859\n",
      "Epoch [26/500] - Train Loss: 0.7133, Train Accuracy: 0.6869\n",
      "Epoch [27/500] - Train Loss: 0.6821, Train Accuracy: 0.6961\n",
      "Epoch [28/500] - Train Loss: 0.6816, Train Accuracy: 0.6970\n",
      "Epoch [29/500] - Train Loss: 0.6801, Train Accuracy: 0.6949\n",
      "Epoch [30/500] - Train Loss: 0.6726, Train Accuracy: 0.6998\n",
      "Epoch [31/500] - Train Loss: 0.6739, Train Accuracy: 0.6979\n",
      "Epoch [32/500] - Train Loss: 0.6649, Train Accuracy: 0.6987\n",
      "Epoch [33/500] - Train Loss: 0.6339, Train Accuracy: 0.7138\n",
      "Epoch [34/500] - Train Loss: 0.6538, Train Accuracy: 0.7057\n",
      "Epoch [35/500] - Train Loss: 0.6259, Train Accuracy: 0.7124\n",
      "Epoch [36/500] - Train Loss: 0.6279, Train Accuracy: 0.7173\n",
      "Epoch [37/500] - Train Loss: 0.6217, Train Accuracy: 0.7198\n",
      "Epoch [38/500] - Train Loss: 0.5959, Train Accuracy: 0.7219\n",
      "Epoch [39/500] - Train Loss: 0.6008, Train Accuracy: 0.7268\n",
      "Epoch [40/500] - Train Loss: 0.6084, Train Accuracy: 0.7196\n",
      "Epoch [41/500] - Train Loss: 0.6063, Train Accuracy: 0.7255\n",
      "Epoch [42/500] - Train Loss: 0.5885, Train Accuracy: 0.7251\n",
      "Epoch [43/500] - Train Loss: 0.5738, Train Accuracy: 0.7344\n",
      "Epoch [44/500] - Train Loss: 0.5721, Train Accuracy: 0.7355\n",
      "Epoch [45/500] - Train Loss: 0.5514, Train Accuracy: 0.7410\n",
      "Epoch [46/500] - Train Loss: 0.5697, Train Accuracy: 0.7325\n",
      "Epoch [47/500] - Train Loss: 0.5716, Train Accuracy: 0.7383\n",
      "Epoch [48/500] - Train Loss: 0.5448, Train Accuracy: 0.7453\n",
      "Epoch [49/500] - Train Loss: 0.5418, Train Accuracy: 0.7447\n",
      "Epoch [50/500] - Train Loss: 0.5543, Train Accuracy: 0.7419\n",
      "Epoch [51/500] - Train Loss: 0.5355, Train Accuracy: 0.7452\n",
      "Epoch [52/500] - Train Loss: 0.5189, Train Accuracy: 0.7494\n",
      "Epoch [53/500] - Train Loss: 0.5509, Train Accuracy: 0.7443\n",
      "Epoch [54/500] - Train Loss: 0.5149, Train Accuracy: 0.7559\n",
      "Epoch [55/500] - Train Loss: 0.5178, Train Accuracy: 0.7555\n",
      "Epoch [56/500] - Train Loss: 0.5067, Train Accuracy: 0.7564\n",
      "Epoch [57/500] - Train Loss: 0.5001, Train Accuracy: 0.7580\n",
      "Epoch [58/500] - Train Loss: 0.4995, Train Accuracy: 0.7610\n",
      "Epoch [59/500] - Train Loss: 0.4840, Train Accuracy: 0.7671\n",
      "Epoch [60/500] - Train Loss: 0.4657, Train Accuracy: 0.7707\n",
      "Epoch [61/500] - Train Loss: 0.4783, Train Accuracy: 0.7675\n",
      "Epoch [62/500] - Train Loss: 0.4786, Train Accuracy: 0.7661\n",
      "Epoch [63/500] - Train Loss: 0.4604, Train Accuracy: 0.7721\n",
      "Epoch [64/500] - Train Loss: 0.4903, Train Accuracy: 0.7594\n",
      "Epoch [65/500] - Train Loss: 0.4564, Train Accuracy: 0.7728\n",
      "Epoch [66/500] - Train Loss: 0.4783, Train Accuracy: 0.7696\n",
      "Epoch [67/500] - Train Loss: 0.4514, Train Accuracy: 0.7784\n",
      "Epoch [68/500] - Train Loss: 0.4555, Train Accuracy: 0.7670\n",
      "Epoch [69/500] - Train Loss: 0.4375, Train Accuracy: 0.7795\n",
      "Epoch [70/500] - Train Loss: 0.4374, Train Accuracy: 0.7777\n",
      "Epoch [71/500] - Train Loss: 0.4525, Train Accuracy: 0.7732\n",
      "Epoch [72/500] - Train Loss: 0.4594, Train Accuracy: 0.7659\n",
      "Epoch [73/500] - Train Loss: 0.4242, Train Accuracy: 0.7834\n",
      "Epoch [74/500] - Train Loss: 0.4756, Train Accuracy: 0.7662\n",
      "Epoch [75/500] - Train Loss: 0.4058, Train Accuracy: 0.7871\n",
      "Epoch [76/500] - Train Loss: 0.3986, Train Accuracy: 0.7951\n",
      "Epoch [77/500] - Train Loss: 0.4064, Train Accuracy: 0.7943\n",
      "Epoch [78/500] - Train Loss: 0.4066, Train Accuracy: 0.7919\n",
      "Epoch [79/500] - Train Loss: 0.4193, Train Accuracy: 0.7817\n",
      "Epoch [80/500] - Train Loss: 0.3896, Train Accuracy: 0.7991\n",
      "Epoch [81/500] - Train Loss: 0.4099, Train Accuracy: 0.7857\n",
      "Epoch [82/500] - Train Loss: 0.4404, Train Accuracy: 0.7756\n",
      "Epoch [83/500] - Train Loss: 0.3882, Train Accuracy: 0.7956\n",
      "Epoch [84/500] - Train Loss: 0.3786, Train Accuracy: 0.7960\n",
      "Epoch [85/500] - Train Loss: 0.4021, Train Accuracy: 0.7885\n",
      "Epoch [86/500] - Train Loss: 0.4106, Train Accuracy: 0.7866\n",
      "Epoch [87/500] - Train Loss: 0.3839, Train Accuracy: 0.7945\n",
      "Epoch [88/500] - Train Loss: 0.3849, Train Accuracy: 0.7956\n",
      "Epoch [89/500] - Train Loss: 0.3628, Train Accuracy: 0.7983\n",
      "Epoch [90/500] - Train Loss: 0.3589, Train Accuracy: 0.8029\n",
      "Epoch [91/500] - Train Loss: 0.3825, Train Accuracy: 0.7971\n",
      "Epoch [92/500] - Train Loss: 0.3726, Train Accuracy: 0.7977\n",
      "Epoch [93/500] - Train Loss: 0.3725, Train Accuracy: 0.7989\n",
      "Epoch [94/500] - Train Loss: 0.3833, Train Accuracy: 0.7979\n",
      "Epoch [95/500] - Train Loss: 0.3612, Train Accuracy: 0.8016\n",
      "Epoch [96/500] - Train Loss: 0.3496, Train Accuracy: 0.8057\n",
      "Epoch [97/500] - Train Loss: 0.3481, Train Accuracy: 0.8060\n",
      "Epoch [98/500] - Train Loss: 0.3689, Train Accuracy: 0.8038\n",
      "Epoch [99/500] - Train Loss: 0.3553, Train Accuracy: 0.8036\n",
      "Epoch [100/500] - Train Loss: 0.3628, Train Accuracy: 0.8029\n",
      "Epoch [101/500] - Train Loss: 0.3383, Train Accuracy: 0.8081\n",
      "Epoch [102/500] - Train Loss: 0.3491, Train Accuracy: 0.8083\n",
      "Epoch [103/500] - Train Loss: 0.3778, Train Accuracy: 0.7961\n",
      "Epoch [104/500] - Train Loss: 0.3274, Train Accuracy: 0.8145\n",
      "Epoch [105/500] - Train Loss: 0.3523, Train Accuracy: 0.8040\n",
      "Epoch [106/500] - Train Loss: 0.3541, Train Accuracy: 0.8051\n",
      "Epoch [107/500] - Train Loss: 0.3250, Train Accuracy: 0.8116\n",
      "Epoch [108/500] - Train Loss: 0.3456, Train Accuracy: 0.8048\n",
      "Epoch [109/500] - Train Loss: 0.3161, Train Accuracy: 0.8157\n",
      "Epoch [110/500] - Train Loss: 0.3281, Train Accuracy: 0.8129\n",
      "Epoch [111/500] - Train Loss: 0.3337, Train Accuracy: 0.8126\n",
      "Epoch [112/500] - Train Loss: 0.4542, Train Accuracy: 0.7764\n",
      "Epoch [113/500] - Train Loss: 0.3196, Train Accuracy: 0.8182\n",
      "Epoch [114/500] - Train Loss: 0.3013, Train Accuracy: 0.8202\n",
      "Epoch [115/500] - Train Loss: 0.3581, Train Accuracy: 0.8063\n",
      "Epoch [116/500] - Train Loss: 0.3248, Train Accuracy: 0.8112\n",
      "Epoch [117/500] - Train Loss: 0.3729, Train Accuracy: 0.8008\n",
      "Epoch [118/500] - Train Loss: 0.3224, Train Accuracy: 0.8164\n",
      "Epoch [119/500] - Train Loss: 0.2995, Train Accuracy: 0.8213\n",
      "Epoch [120/500] - Train Loss: 0.2990, Train Accuracy: 0.8140\n",
      "Epoch [121/500] - Train Loss: 0.2893, Train Accuracy: 0.8188\n",
      "Epoch [122/500] - Train Loss: 0.3464, Train Accuracy: 0.8093\n",
      "Epoch [123/500] - Train Loss: 0.3459, Train Accuracy: 0.8062\n",
      "Epoch [124/500] - Train Loss: 0.3472, Train Accuracy: 0.8035\n",
      "Epoch [125/500] - Train Loss: 0.3377, Train Accuracy: 0.8124\n",
      "Epoch [126/500] - Train Loss: 0.3221, Train Accuracy: 0.8125\n",
      "Epoch [127/500] - Train Loss: 0.2864, Train Accuracy: 0.8228\n",
      "Epoch [128/500] - Train Loss: 0.3256, Train Accuracy: 0.8176\n",
      "Epoch [129/500] - Train Loss: 0.3938, Train Accuracy: 0.7896\n",
      "Epoch [130/500] - Train Loss: 0.3218, Train Accuracy: 0.8156\n",
      "Epoch [131/500] - Train Loss: 0.3240, Train Accuracy: 0.8180\n",
      "Epoch [132/500] - Train Loss: 0.3030, Train Accuracy: 0.8200\n",
      "Epoch [133/500] - Train Loss: 0.2945, Train Accuracy: 0.8253\n",
      "Epoch [134/500] - Train Loss: 0.3197, Train Accuracy: 0.8123\n",
      "Epoch [135/500] - Train Loss: 0.2865, Train Accuracy: 0.8293\n",
      "Epoch [136/500] - Train Loss: 0.2647, Train Accuracy: 0.8342\n",
      "Epoch [137/500] - Train Loss: 0.2780, Train Accuracy: 0.8264\n",
      "Epoch [138/500] - Train Loss: 0.2994, Train Accuracy: 0.8164\n",
      "Epoch [139/500] - Train Loss: 0.3107, Train Accuracy: 0.8104\n",
      "Epoch [140/500] - Train Loss: 0.3123, Train Accuracy: 0.8192\n",
      "Epoch [141/500] - Train Loss: 0.3281, Train Accuracy: 0.8070\n",
      "Epoch [142/500] - Train Loss: 0.2880, Train Accuracy: 0.8264\n",
      "Epoch [143/500] - Train Loss: 0.2698, Train Accuracy: 0.8353\n",
      "Epoch [144/500] - Train Loss: 0.3237, Train Accuracy: 0.8169\n",
      "Epoch [145/500] - Train Loss: 0.2963, Train Accuracy: 0.8213\n",
      "Epoch [146/500] - Train Loss: 0.2726, Train Accuracy: 0.8286\n",
      "Epoch [147/500] - Train Loss: 0.2723, Train Accuracy: 0.8285\n",
      "Epoch [148/500] - Train Loss: 0.2726, Train Accuracy: 0.8274\n",
      "Epoch [149/500] - Train Loss: 0.2500, Train Accuracy: 0.8363\n",
      "Epoch [150/500] - Train Loss: 0.2396, Train Accuracy: 0.8370\n",
      "Epoch [151/500] - Train Loss: 0.2348, Train Accuracy: 0.8408\n",
      "Epoch [152/500] - Train Loss: 0.2369, Train Accuracy: 0.8323\n",
      "Epoch [153/500] - Train Loss: 0.2383, Train Accuracy: 0.8372\n",
      "Epoch [154/500] - Train Loss: 0.2615, Train Accuracy: 0.8323\n",
      "Epoch [155/500] - Train Loss: 0.2478, Train Accuracy: 0.8327\n",
      "Epoch [156/500] - Train Loss: 0.2367, Train Accuracy: 0.8331\n",
      "Epoch [157/500] - Train Loss: 0.2513, Train Accuracy: 0.8328\n",
      "Epoch [158/500] - Train Loss: 0.2322, Train Accuracy: 0.8410\n",
      "Epoch [159/500] - Train Loss: 0.2326, Train Accuracy: 0.8385\n",
      "Epoch [160/500] - Train Loss: 0.2268, Train Accuracy: 0.8383\n",
      "Epoch [161/500] - Train Loss: 0.2249, Train Accuracy: 0.8412\n",
      "Epoch [162/500] - Train Loss: 0.2272, Train Accuracy: 0.8377\n",
      "Epoch [163/500] - Train Loss: 0.2318, Train Accuracy: 0.8384\n",
      "Epoch [164/500] - Train Loss: 0.2253, Train Accuracy: 0.8394\n",
      "Epoch [165/500] - Train Loss: 0.2302, Train Accuracy: 0.8375\n",
      "Epoch [166/500] - Train Loss: 0.2254, Train Accuracy: 0.8467\n",
      "Epoch [167/500] - Train Loss: 0.2228, Train Accuracy: 0.8395\n",
      "Epoch [168/500] - Train Loss: 0.2243, Train Accuracy: 0.8380\n",
      "Epoch [169/500] - Train Loss: 0.2420, Train Accuracy: 0.8415\n",
      "Epoch [170/500] - Train Loss: 0.2347, Train Accuracy: 0.8388\n",
      "Epoch [171/500] - Train Loss: 0.2311, Train Accuracy: 0.8389\n",
      "Epoch [172/500] - Train Loss: 0.2323, Train Accuracy: 0.8399\n",
      "Epoch [173/500] - Train Loss: 0.2329, Train Accuracy: 0.8380\n",
      "Epoch [174/500] - Train Loss: 0.2360, Train Accuracy: 0.8340\n",
      "Epoch [175/500] - Train Loss: 0.2346, Train Accuracy: 0.8380\n",
      "Epoch [176/500] - Train Loss: 0.2293, Train Accuracy: 0.8413\n",
      "Epoch [177/500] - Train Loss: 0.2287, Train Accuracy: 0.8380\n",
      "Epoch [178/500] - Train Loss: 0.2227, Train Accuracy: 0.8428\n",
      "Epoch [179/500] - Train Loss: 0.2900, Train Accuracy: 0.8262\n",
      "Epoch [180/500] - Train Loss: 0.2664, Train Accuracy: 0.8285\n",
      "Epoch [181/500] - Train Loss: 0.2478, Train Accuracy: 0.8344\n",
      "Epoch [182/500] - Train Loss: 0.2378, Train Accuracy: 0.8350\n",
      "Epoch [183/500] - Train Loss: 0.2327, Train Accuracy: 0.8357\n",
      "Epoch [184/500] - Train Loss: 0.2226, Train Accuracy: 0.8434\n",
      "Epoch [185/500] - Train Loss: 0.2330, Train Accuracy: 0.8384\n",
      "Epoch [186/500] - Train Loss: 0.2413, Train Accuracy: 0.8332\n",
      "Early stopping at epoch 186\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Block 5: Model Training and Prediction\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if len(train_features) > 0:\n",
    "    if len(train_features) != len(train_labels):\n",
    "        raise ValueError(\"Number of train features and labels do not match.\")\n",
    "\n",
    "    # Reshape the data to match the input shape of the CNN\n",
    "    train_data = torch.tensor(train_features, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.long, device=device)\n",
    "\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, input_size, num_classes):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 = nn.Conv1d(1, 256, kernel_size=7, padding=3)\n",
    "            self.bn1 = nn.BatchNorm1d(256)  # Added batch normalization\n",
    "            self.relu1 = nn.ReLU()\n",
    "            self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "            self.conv2 = nn.Conv1d(256, 512, kernel_size=5, padding=2)\n",
    "            self.bn2 = nn.BatchNorm1d(512)  # Added batch normalization\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "            self.conv3 = nn.Conv1d(512, 1024, kernel_size=3, padding=1)\n",
    "            self.bn3 = nn.BatchNorm1d(1024)  # Added batch normalization\n",
    "            self.relu3 = nn.ReLU()\n",
    "            self.pool3 = nn.AdaptiveAvgPool1d(1)\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc1 = nn.Linear(1024, 512)\n",
    "            self.relu4 = nn.ReLU()\n",
    "            self.dropout1 = nn.Dropout(0.8)\n",
    "            self.fc2 = nn.Linear(512, 256)\n",
    "            self.relu5 = nn.ReLU()\n",
    "            self.dropout2 = nn.Dropout(0.6)\n",
    "            self.fc3 = nn.Linear(256, 128)\n",
    "            self.relu6 = nn.ReLU()\n",
    "            self.dropout3 = nn.Dropout(0.4)\n",
    "            self.fc4 = nn.Linear(128, num_classes)\n",
    "            self.dropout4 = nn.Dropout(0.3)\n",
    "\n",
    "            # Add L2 regularization\n",
    "            self.conv1.weight_decay = 1e-3\n",
    "            self.conv2.weight_decay = 1e-3\n",
    "            self.conv3.weight_decay = 1e-3\n",
    "            self.fc1.weight_decay = 1e-3\n",
    "            self.fc2.weight_decay = 1e-3\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)  # Added batch normalization\n",
    "            x = self.relu1(x)\n",
    "            x = self.pool1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.bn2(x)  # Added batch normalization\n",
    "            x = self.relu2(x)\n",
    "            x = self.pool2(x)\n",
    "            x = self.conv3(x)\n",
    "            x = self.bn3(x)  # Added batch normalization\n",
    "            x = self.relu3(x)\n",
    "            x = self.pool3(x)\n",
    "            x = self.flatten(x)\n",
    "            x = self.fc1(x)\n",
    "            x = self.relu4(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.relu5(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc3(x)\n",
    "            x = self.relu6(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = self.fc4(x)\n",
    "            x = self.dropout4(x)\n",
    "            return x\n",
    "\n",
    "    input_size = train_data.shape[2]\n",
    "    num_classes = 4\n",
    "    num_epochs = 500  # Increased the number of epochs\n",
    "    batch_size = 512\n",
    "\n",
    "    model = CNN(input_size, num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    early_stopping_counter = 0\n",
    "    patience = 20  # Increased early stopping patience\n",
    "\n",
    "    # Create data loader for the entire dataset\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_accuracy = 0.0\n",
    "\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * batch_data.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_accuracy += (predicted == batch_labels).sum().item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_accuracy = train_accuracy / len(train_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "        # Early stopping based on training accuracy\n",
    "        if train_accuracy > best_accuracy:\n",
    "            best_accuracy = train_accuracy\n",
    "            early_stopping_counter = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    # Generate submission.csv\n",
    "    test_features = torch.tensor(test_features, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_features)\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "        predicted_labels = predicted_labels.cpu().tolist()\n",
    "\n",
    "    submission = pd.DataFrame({'id': range(len(predicted_labels)), 'category': predicted_labels})\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "else:\n",
    "    print(\"No training features available. Please check the data.\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
